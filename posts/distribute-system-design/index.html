<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>分布式系统设计思路总览 | 杏仁儿的博客</title><meta name=keywords content="Distribute System,Design,Paxos,Raft,CAP"><meta name=description content="CAP 定理 CAP 原则又称 CAP 定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Part"><meta name=author content><link rel=canonical href=https://xinrea.cn/posts/distribute-system-design/><link crossorigin=anonymous href=/assets/css/stylesheet.min.89887dc5a832239ce37df4f854054cc5017c6a9d4bcb7374aaea36fdf934472a.css integrity="sha256-iYh9xagyI5zjffT4VAVMxQF8ap1Ly3N0quo2/fk0Ryo=" rel="preload stylesheet" as=style><link rel=icon href=https://xinrea.cn/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://xinrea.cn/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://xinrea.cn/favicon-32x32.png><link rel=apple-touch-icon href=https://xinrea.cn/apple-touch-icon.png><link rel=mask-icon href=https://xinrea.cn/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
  const config = {
    startOnLoad:true,
    theme: 'forest',
    flowchart: {
        useMaxWidth:false,
        htmlLabels:true
        }
  };
  mermaid.initialize(config);
  window.mermaid = mermaid;
  window.onload = () => {
    window.mermaid.init(undefined, document.querySelectorAll('.language-mermaid'));
  };
</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-YDJZ0WLTD6"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-YDJZ0WLTD6",{anonymize_ip:!1})}</script><meta property="og:title" content="分布式系统设计思路总览"><meta property="og:description" content="CAP 定理 CAP 原则又称 CAP 定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Part"><meta property="og:type" content="article"><meta property="og:url" content="https://xinrea.cn/posts/distribute-system-design/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-24T00:00:00+00:00"><meta property="article:modified_time" content="2023-09-19T23:02:35+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="分布式系统设计思路总览"><meta name=twitter:description content="CAP 定理 CAP 原则又称 CAP 定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Part"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://xinrea.cn/posts/"},{"@type":"ListItem","position":3,"name":"分布式系统设计思路总览","item":"https://xinrea.cn/posts/distribute-system-design/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"分布式系统设计思路总览","name":"分布式系统设计思路总览","description":"CAP 定理 CAP 原则又称 CAP 定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Part","keywords":["Distribute System","Design","Paxos","Raft","CAP"],"articleBody":"CAP 定理 CAP 原则又称 CAP 定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）。CAP 原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾。\n显然分区容错性 P 是必须要满足的，因为在分布式系统中网络是不可靠的，这意味着系统很可能会被分割成多个区域（也等效于节点失效，因为客观上无法分辨是网络原因还是节点原因）。如果此时不能保证分区容错性，那么也就是所一旦发生分区（或者说节点失效），分布式系统就无法正常工作了；由于分布式系统中含有许多节点，这会导致系统的故障率远远大于单体，这显然是不能接受的，与使用分布式系统的初衷背道而驰。\n因此在满足 P 的前提下，也就是说当节点失效时，C 和 A 之间的取舍就成为了分布式系统设计中的核心问题。在实际的分布式系统中，我们看到的所有设计都是围绕这两点来进行的。\n从单体到多体 假设我们有一个服务 A 运行在单个节点上，它的功能是 resp = f(req)。这个服务十分重要，我们希望它能够 7x24 小时运行，不间断地提供服务。那么我们就需要考虑如何保证这个服务的可用性。\n如果服务 A 是无状态的，也就是说服务 A 的响应 resp 只取决于 req，而与之前的请求无关，那么我们可以通过复制的方式来保证服务的可用性。我们可以在任意多的节点上运行服务 A，然后将请求以任意形式分发到可用的节点上，从而保证服务的可用性。这类无状态服务处理起来比较简单，因此在微服务拆分时，我们尽量将服务拆分成无状态的服务，这样可以大大降低系统的复杂度。\n但是，在实际中比较重要的服务往往是有状态的，也就是说服务 A 的响应 resp 不仅取决于 req，还取决于之前的请求。\n1. 备份（Master-Slave/Leader-Follower/Primary-Replica） 针对有状态服务，最简单的方法就是备份，我们可以在另一个节点上运行一个服务 B，它的功能也是 resp = f(req)。当请求到达时，我们将请求同时发送到 A 和 B，然后将 A 或者 B 的响应返回给客户端。这样，B 节点的状态与 A 节点一致，当 A 节点发生故障时，我们可以切换到 B 节点，让其接替服务。我们称 A、B 这些节点组成了一个集群。\n想想这样一个场景，请求到达集群后，由于网络原因（或者故障，集群视角无法区分），其中一个节点没有响应这个请求，那么这意味着这个节点的状态与其他节点不一致，这时我们应该怎么办呢？\n1.1 拥抱一致性 保证强一致性（线性一致性）可以想到以下几种处理方法\n对外响应处理失败，那么对于集群内的 N 个节点，一旦有一个节点出现错误，其他所有节点都要撤销操作，保证与失败的节点状态一致 不断重试，最终对外响应处理成功。但是这样会导致请求的响应时间变长，很有可能请求永远也不会成功 对外响应处理成功，剔除故障节点，将故障节点定义到一致性范围之外。这样做的问题是，如果故障节点恢复了，那么它的状态就会与其他节点不一致，一致性要求集群内节点必须与集群内其他节点状态一致，这意味着故障节点永远无法恢复；如果要加入或者恢复节点，都要停止对外服务，等待集群内所有节点状态一致再对外开放。遗憾的是，节点故障在分布式系统中是常态，这样做显然是不可行的 可见为了保证一致性，我们必须要牺牲可用性，这与 CAP 原则是一致的。\n1.2 拥抱可用性 如果我们选择可用性，那么我们可以采取以下处理方法\n如果部分节点处理成功，那么就对外响应处理成功 这里的 部分 可以是一个，可以是大部分，可以是权重最高的几个，也可以是随机的几个，可以自由选择合适的策略。\n这样做的问题是，后续请求发送到状态不一致的节点上时，处理很可能是错误的；好处是可用性大大提升，至少请求都及时处理了。\n虽然能够完成请求，但是如果对请求的处理都是错的，那么可用性再高也没有意义；还好这里 错 的定义还有可商量的余地，使得我们可以在一致性和可用性之间做出权衡。\n1.3 拥抱最终一致性 其实在强一致性（线性一致性）到最终一致性之间，还有许多不同等级的一致性；在此直接考虑要求最低的最终一致性，是因为在实际中最多考虑的还是可用性，因此在一致性方面牺牲了许多。\n最终一致性是指，在没有新的请求时，集群内的节点最终会达到一致状态，显然这里的时间差越短越好。实际上，可以结合 1.1 和 1.2 的方法来实现这一点。\n回顾以下两种方法：\n[一致性] 对外响应处理成功，剔除故障节点，将故障节点定义到一致性范围之外 [可用性] 如果部分节点处理成功，那么就对外响应处理成功 一致性方面，我们定义符合最终一致性的节点集合为 E，其中的强一致性节点组成集合 C，那么有\n$$C \\subseteq E$$\n集群的状态为其中状态最新的节点状态。根据请求的类别，我们可以将请求分为两类：\n强一致性请求 CR：例如涉及到写操作的请求，这类请求只能发送到强一致性节点集合 C 上，原因是：写操作需要改变集群的状态，将集群看作状态机，初始状态 Sk 在收到操作 Wj 后，状态改变为 Sk+1；同理要让状态最终一致，那么要求 Wj 必须发送到状态为 Sk 的节点上，这样才能保证集群最终状态一致；即涉及到状态改变的操作，只能发送到 C 上 最终一致性请求 ER：首先这类请求不涉及集群状态变化，可以发送到最终一致性节点集合 E 上；其次要求可以容忍读取到旧数据 我们来看看强一致性节点集合 C 的性质：\n从 1.1 中得知，对于强一致性节点集合 C，C 中元素越多，集群处理强一致性请求的可用性越低\n其中红线假设单节点可用性 99%；蓝线假设单节点可用性 99.9%；绿线假设单节点可用性 99.99%；紫线为可用性 90% 线。\n可见扩展 C 将使得集群可用性急剧下跌，且十分依靠单节点可用性。另一方面，在实现时，需要将 CR 发送到 C 中的每一个节点，造成了大量的带宽浪费；其次，为了维护 C，需要实现一个中间件用于接收分发 CR，并在节点故障时将其移出 C，而且需要统计所有 C 中节点的响应结果，集群对于 CR 的吞吐性能为 C 中各节点性能的最小值，造成了吞吐性能的急剧下滑；最后，中间件如果出现问题会导致集群不可用，因此中间件还需要集群化。\n由于以上的众多原因，实际上在集群中 C 往往只包含一个节点 C0，用于集群处理 CR 请求，这样做有以下优缺点：\n优点：\nE 节点同步简单，集群状态等同于 C0 的状态；其他节点与 C0 同步即可 CR 请求处理简单，直接交给 C0 处理即可 缺点：\nC0 挂掉会导致集群无法对 CR 请求提供服务 分布式中没有十全十美的解决方案，都需要有所取舍，但最终有很多项目都选用了 C 只包含一个节点的形式。例如 MySQL、Redis、Kafka、Zookeeper、使用 Raft 机制的其他项目等等，都使用了这一设计思想。\n当然要使用这种设计，得解决一个核心问题，也就是弥补该设计的缺点：C0挂掉会导致集群无法对 CR 请求提供服务。显然我们不可能启用全新的节点来替代，而集群中有着许多接近 C0 的状态的最终一致性节点，因此可以从它们中选出新的 C0。当然，如果选出的节点状态与原来的 C0 不同，则会导致数据的丢失，好在有着一系列机制来保证这些节点的最终一致性，而且 C0 不可用时集群状态不会再更新，相当于提供了一个等待同步的机会，因此可以尽可能避免数据丢失情况的产生，这一点后续再详细阐述。\n因此现在我们遇到了一个新问题，当 C0 挂掉时，该如何切换节点来替代 C0。\n1.4 如何切换 1.4.1 人工干预 切换的方法有很多种，其中最简单的方法就是人工干预，当 A 节点发生故障时，我们手动将请求切换到 B 节点。这种方法的缺点是服务无法使用，直到人工干预完成。这种方法看起来很蠢，但是如果不是使用在故障处理上，还是有可取之处的。\n例如 MySQL 集群的迁移就可以用到这种方式；首先将新的节点加入最终一致性节点集合 E，也就是作为 Slave 节点来同步 Master 节点的数据。待到数据同步完成，ER 操作实际上已经可以切换到新节点上了；最后手动在新节点中启用一个 Master 节点，替换旧 Master 节点，然后将 CR 请求切换到新 Master 节点即可。\n显然最后切换 Master 节点时与 C0 节点故障的处理等效，因此集群无法提供服务。\n1.4.2 哨兵机制 为了自动化完成前面提到的人工干预方法，引入了哨兵机制。其本质上跟人工干预相同，不过是使用一个服务来替代人完成这一操作。优点是无需在原有服务上做修改，哨兵服务与原服务独立运行。\n显然，哨兵服务的可用性也需要用集群来保证，因此 Redis 哨兵模式架构如下所示：\ngraph TD subgraph Sentinel A1(Sentinel Leader) \u003c--\u003e A2(Sentinel 1) A3(Sentinel 2) \u003c--\u003e A2 A3(Sentinel 2) \u003c--\u003e A1 end subgraph Redis B(Master) \u003c--\u003e C(Slave 1) B \u003c--\u003e D(Slave 2) end Sentinel --\u003e Redis 哨兵服务监控所有 Redis 节点的状态，以便当 Master 节点宕机后进行自动切换。这样就解决了 Redis Master 节点宕机后新 Matser 的切换问题。\n但是和 Redis 集群一样，哨兵服务在执行各种操作时，执行操作的主体只能是一个节点（Leader），其他节点作为备份（Follower），以达成一定的一致性和可用性。那么，当 Sentinel Leader 挂掉后，该如何切换新的 Leader 节点呢？如果还是使用哨兵机制的话，那么需要启用 Sentinel 的 Sentinel 服务，最终会无限套娃下去。\n为了打破这一套娃循环，我们需要集群本身具有 C0 节点的切换机制，不能依赖外部服务。\n1.4.3 分布式共识算法 前面在描述哨兵集群中的节点时，使用的是 Leader/Follower 而不是 Master/Slave，这是因为哨兵集群使用 Raft 算法实现了 C0 的自动切换。通常我们将使用选举算法实现的 C0 称为 Leader，实际上集群视角来看都为主从模式，只是“主”的故障迁移使用了不同的机制来实现；避免使用 Master 和 Slave 等词汇也有政治正确的体现（例如 GitHub 将主分支默认名称 Master 更改为 Main)。\n首先要说明，此处不讨论拜占庭将军问题，因为我们认为对集群中节点具有完全的控制权，因此集群内部的节点均按照规定的机制运行，出现的所有错误情况均是可预测的。\n因此这里讨论的分布式共识没有 BTC 等区块链项目的共识机制严格，但仍需考虑各种异常情况的处理；在这一点也能窥见区块链的魅力，在完全混沌和不可信的环境中构建一份共识。\nPaxos 算法 提到分布式共识算法就得从 Paxos 算法 说起。Paxos 算法模拟了一个小岛上通过决议的流程，一个值的确定需要多数人发起提案（Prepare 阶段）后，得到多数人的同意（Accept 阶段）。其中每个人都要维护一个Proposal ID，已确保只处理自身视角里最新的 Proposal，Proposal ID 将会在 Prepare 和 Accept 阶段中更新。\n这样的 Basic Paxos 算法的问题在于，只要多数人就可发起提案，因此很可能前一个提案还未处理完便发起了新的提案，导致 Accept 阶段有些人得知了新的提案便不再处理旧提案，使得旧提案无法得到多数人的确认，此时若再次发起提案，那么刚刚的新提案也会遇到同样的问题，造成活锁；同时，这样的流程仅能确定一个值，无法满足实际需求。\n为了解决以上 Basic Paxos 算法的缺陷，很自然的提出了 Multi-Paxos 算法。为了解决只能确定一个值的问题，我们对每一个需要决定的值都使用一次 Paxos 算法；为了解决活锁的问题，我们限制提案只能由一个人发起，这个人叫做 Leader，代价是损失了一定的可用性，一旦 Leader 宕机算法将无法正常进行。\n同时，由于 Leader 只有一个且算法将多次进行，那么在一开始提前使用 Paxos 算法确定好 Leader 后，后续的 Prepare 阶段都可以省略掉了；当 Leader 宕机后使用 Paxos 算法再选一个出来就可以了。这样还解决了算法 Prepare 阶段网络 IO 的耗时，提高了算法的效率。\nLeader 的选举还是使用的 Basic Paxos 算法，但是由于目的明确且易于控制，不会连续产生多个 Proposal 导致活锁。\nRaft 算法 ","wordCount":"4452","inLanguage":"en","datePublished":"2023-06-24T00:00:00Z","dateModified":"2023-09-19T23:02:35+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://xinrea.cn/posts/distribute-system-design/"},"publisher":{"@type":"Organization","name":"杏仁儿的博客","logo":{"@type":"ImageObject","url":"https://xinrea.cn/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://xinrea.cn accesskey=h title="杏仁儿的博客 (Alt + H)">杏仁儿的博客</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://xinrea.cn/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://xinrea.cn/tags title=Tags><span>Tags</span></a></li><li><a href=https://xinrea.cn/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://xinrea.cn/projects title=Projects><span>Projects</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://xinrea.cn>Home</a>&nbsp;»&nbsp;<a href=https://xinrea.cn/posts/>Posts</a></div><h1 class=post-title>分布式系统设计思路总览<sup><span class=entry-isdraft>&nbsp;&nbsp;[draft]</span></sup></h1><div class=post-meta><span title='2023-06-24 00:00:00 +0000 UTC'>June 24, 2023</span>&nbsp;·&nbsp;<span title='2023-09-19 23:02:35 +0800 +0800'>updated September 19, 2023</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;4452 words&nbsp;|&nbsp;<a href=https://github.com/Xinrea/Xinrea.github.io/tree/main/content/posts/distribute-system-design.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#cap-%e5%ae%9a%e7%90%86 aria-label="CAP 定理">CAP 定理</a></li><li><a href=#%e4%bb%8e%e5%8d%95%e4%bd%93%e5%88%b0%e5%a4%9a%e4%bd%93 aria-label=从单体到多体>从单体到多体</a><ul><li><a href=#1-%e5%a4%87%e4%bb%bdmaster-slaveleader-followerprimary-replica aria-label="1. 备份（Master-Slave/Leader-Follower/Primary-Replica）">1. 备份（Master-Slave/Leader-Follower/Primary-Replica）</a><ul><li><a href=#11-%e6%8b%a5%e6%8a%b1%e4%b8%80%e8%87%b4%e6%80%a7 aria-label="1.1 拥抱一致性">1.1 拥抱一致性</a></li><li><a href=#12-%e6%8b%a5%e6%8a%b1%e5%8f%af%e7%94%a8%e6%80%a7 aria-label="1.2 拥抱可用性">1.2 拥抱可用性</a></li><li><a href=#13-%e6%8b%a5%e6%8a%b1%e6%9c%80%e7%bb%88%e4%b8%80%e8%87%b4%e6%80%a7 aria-label="1.3 拥抱最终一致性">1.3 拥抱最终一致性</a></li><li><a href=#14-%e5%a6%82%e4%bd%95%e5%88%87%e6%8d%a2 aria-label="1.4 如何切换">1.4 如何切换</a><ul><li><a href=#141-%e4%ba%ba%e5%b7%a5%e5%b9%b2%e9%a2%84 aria-label="1.4.1 人工干预">1.4.1 人工干预</a></li><li><a href=#142-%e5%93%a8%e5%85%b5%e6%9c%ba%e5%88%b6 aria-label="1.4.2 哨兵机制">1.4.2 哨兵机制</a></li><li><a href=#143-%e5%88%86%e5%b8%83%e5%bc%8f%e5%85%b1%e8%af%86%e7%ae%97%e6%b3%95 aria-label="1.4.3 分布式共识算法">1.4.3 分布式共识算法</a></li></ul></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h2 id=cap-定理>CAP 定理<a hidden class=anchor aria-hidden=true href=#cap-定理>#</a></h2><p>CAP 原则又称 CAP 定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）。CAP 原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾。</p><p>显然分区容错性 P 是必须要满足的，因为在分布式系统中网络是不可靠的，这意味着系统很可能会被分割成多个区域（也等效于节点失效，因为客观上无法分辨是网络原因还是节点原因）。如果此时不能保证分区容错性，那么也就是所一旦发生分区（或者说节点失效），分布式系统就无法正常工作了；由于分布式系统中含有许多节点，这会导致系统的故障率远远大于单体，这显然是不能接受的，与使用分布式系统的初衷背道而驰。</p><p>因此在满足 P 的前提下，也就是说当节点失效时，C 和 A 之间的取舍就成为了分布式系统设计中的核心问题。在实际的分布式系统中，我们看到的所有设计都是围绕这两点来进行的。</p><h2 id=从单体到多体>从单体到多体<a hidden class=anchor aria-hidden=true href=#从单体到多体>#</a></h2><p>假设我们有一个服务 A 运行在单个节点上，它的功能是 <code>resp = f(req)</code>。这个服务十分重要，我们希望它能够 7x24 小时运行，不间断地提供服务。那么我们就需要考虑如何保证这个服务的可用性。</p><p>如果服务 A 是无状态的，也就是说服务 A 的响应 <code>resp</code> 只取决于 <code>req</code>，而与之前的请求无关，那么我们可以通过复制的方式来保证服务的可用性。我们可以在任意多的节点上运行服务 A，然后将请求以任意形式分发到可用的节点上，从而保证服务的可用性。这类无状态服务处理起来比较简单，因此在微服务拆分时，我们尽量将服务拆分成无状态的服务，这样可以大大降低系统的复杂度。</p><p>但是，在实际中比较重要的服务往往是有状态的，也就是说服务 A 的响应 <code>resp</code> 不仅取决于 <code>req</code>，还取决于之前的请求。</p><h3 id=1-备份master-slaveleader-followerprimary-replica>1. 备份（Master-Slave/Leader-Follower/Primary-Replica）<a hidden class=anchor aria-hidden=true href=#1-备份master-slaveleader-followerprimary-replica>#</a></h3><p>针对有状态服务，最简单的方法就是备份，我们可以在另一个节点上运行一个服务 B，它的功能也是 <code>resp = f(req)</code>。当请求到达时，我们将请求同时发送到 A 和 B，然后将 A 或者 B 的响应返回给客户端。这样，B 节点的状态与 A 节点一致，当 A 节点发生故障时，我们可以切换到 B 节点，让其接替服务。我们称 A、B 这些节点组成了一个集群。</p><p>想想这样一个场景，请求到达集群后，由于网络原因（或者故障，集群视角无法区分），其中一个节点没有响应这个请求，那么这意味着这个节点的状态与其他节点不一致，这时我们应该怎么办呢？</p><h4 id=11-拥抱一致性>1.1 拥抱一致性<a hidden class=anchor aria-hidden=true href=#11-拥抱一致性>#</a></h4><p>保证强一致性（线性一致性）可以想到以下几种处理方法</p><ul><li>对外响应处理失败，那么对于集群内的 N 个节点，一旦有一个节点出现错误，其他所有节点都要撤销操作，保证与失败的节点状态一致</li><li>不断重试，最终对外响应处理成功。但是这样会导致请求的响应时间变长，很有可能请求永远也不会成功</li><li>对外响应处理成功，剔除故障节点，将故障节点定义到一致性范围之外。这样做的问题是，如果故障节点恢复了，那么它的状态就会与其他节点不一致，一致性要求集群内节点必须与集群内其他节点状态一致，这意味着故障节点永远无法恢复；如果要加入或者恢复节点，都要停止对外服务，等待集群内所有节点状态一致再对外开放。遗憾的是，节点故障在分布式系统中是常态，这样做显然是不可行的</li></ul><p>可见为了保证一致性，我们必须要牺牲可用性，这与 CAP 原则是一致的。</p><h4 id=12-拥抱可用性>1.2 拥抱可用性<a hidden class=anchor aria-hidden=true href=#12-拥抱可用性>#</a></h4><p>如果我们选择可用性，那么我们可以采取以下处理方法</p><ul><li>如果部分节点处理成功，那么就对外响应处理成功</li></ul><p>这里的 <code>部分</code> 可以是一个，可以是大部分，可以是权重最高的几个，也可以是随机的几个，可以自由选择合适的策略。</p><p>这样做的问题是，后续请求发送到状态不一致的节点上时，处理很可能是错误的；好处是可用性大大提升，至少请求都及时处理了。</p><p>虽然能够完成请求，但是如果对请求的处理都是错的，那么可用性再高也没有意义；还好这里 <code>错</code> 的定义还有可商量的余地，使得我们可以在一致性和可用性之间做出权衡。</p><h4 id=13-拥抱最终一致性>1.3 拥抱最终一致性<a hidden class=anchor aria-hidden=true href=#13-拥抱最终一致性>#</a></h4><blockquote><p>其实在强一致性（线性一致性）到最终一致性之间，还有许多不同等级的一致性；在此直接考虑要求最低的最终一致性，是因为在实际中最多考虑的还是可用性，因此在一致性方面牺牲了许多。</p></blockquote><p>最终一致性是指，在没有新的请求时，集群内的节点最终会达到一致状态，显然这里的时间差越短越好。实际上，可以结合 1.1 和 1.2 的方法来实现这一点。</p><p>回顾以下两种方法：</p><ul><li>[一致性] 对外响应处理成功，剔除故障节点，将故障节点定义到一致性范围之外</li><li>[可用性] 如果部分节点处理成功，那么就对外响应处理成功</li></ul><p>一致性方面，我们定义符合最终一致性的节点集合为 E，其中的强一致性节点组成集合 C，那么有</p><p>$$C \subseteq E$$</p><p>集群的状态为其中状态最新的节点状态。根据请求的类别，我们可以将请求分为两类：</p><ul><li>强一致性请求 CR：例如涉及到写操作的请求，这类请求只能发送到强一致性节点集合 C 上，原因是：写操作需要改变集群的状态，将集群看作状态机，初始状态 S<sub>k</sub> 在收到操作 W<sub>j</sub> 后，状态改变为 S<sub>k+1</sub>；同理要让状态最终一致，那么要求 W<sub>j</sub> 必须发送到状态为 S<sub>k</sub> 的节点上，这样才能保证集群最终状态一致；即涉及到状态改变的操作，只能发送到 C 上</li><li>最终一致性请求 ER：首先这类请求不涉及集群状态变化，可以发送到最终一致性节点集合 E 上；其次要求可以容忍读取到旧数据</li></ul><p>我们来看看强一致性节点集合 C 的性质：</p><p><em>从 1.1 中得知，对于强一致性节点集合 C，C 中元素越多，集群处理强一致性请求的可用性越低</em></p><center><iframe src=https://www.desmos.com/calculator/7jawmsfpsx?embed width=500 height=500 style="border:1px solid #ccc" frameborder=0></iframe></center><p>其中<font color=Red>红线</font>假设单节点可用性 99%；<font color=Blue>蓝线</font>假设单节点可用性 99.9%；<font color=Green>绿线</font>假设单节点可用性 99.99%；<font color=Purple>紫线</font>为可用性 90% 线。</p><p>可见扩展 C 将使得集群可用性急剧下跌，且十分依靠单节点可用性。另一方面，在实现时，需要将 CR 发送到 C 中的每一个节点，造成了大量的带宽浪费；其次，为了维护 C，需要实现一个中间件用于接收分发 CR，并在节点故障时将其移出 C，而且需要统计所有 C 中节点的响应结果，集群对于 CR 的吞吐性能为 C 中各节点性能的最小值，造成了吞吐性能的急剧下滑；最后，中间件如果出现问题会导致集群不可用，因此中间件还需要集群化。</p><p>由于以上的众多原因，实际上在集群中 C 往往只包含一个节点 C<sub>0</sub>，用于集群处理 CR 请求，这样做有以下优缺点：</p><p>优点：</p><ul><li>E 节点同步简单，集群状态等同于 C<sub>0</sub> 的状态；其他节点与 C<sub>0</sub> 同步即可</li><li>CR 请求处理简单，直接交给 C<sub>0</sub> 处理即可</li></ul><p>缺点：</p><ul><li>C<sub>0</sub> 挂掉会导致集群无法对 CR 请求提供服务</li></ul><p>分布式中没有十全十美的解决方案，都需要有所取舍，但最终有很多项目都选用了 C 只包含一个节点的形式。例如 MySQL、Redis、Kafka、Zookeeper、使用 Raft 机制的其他项目等等，都使用了这一设计思想。</p><p>当然要使用这种设计，得解决一个核心问题，也就是弥补该设计的缺点：C<sub>0</sub>挂掉会导致集群无法对 CR 请求提供服务。显然我们不可能启用全新的节点来替代，而集群中有着许多接近 C<sub>0</sub> 的状态的最终一致性节点，因此可以从它们中选出新的 C<sub>0</sub>。当然，如果选出的节点状态与原来的 C<sub>0</sub> 不同，则会导致数据的丢失，好在有着一系列机制来保证这些节点的最终一致性，而且 C<sub>0</sub> 不可用时集群状态不会再更新，相当于提供了一个等待同步的机会，因此可以尽可能避免数据丢失情况的产生，这一点后续再详细阐述。</p><p>因此现在我们遇到了一个新问题，当 C<sub>0</sub> 挂掉时，该如何切换节点来替代 C<sub>0</sub>。</p><h4 id=14-如何切换>1.4 如何切换<a hidden class=anchor aria-hidden=true href=#14-如何切换>#</a></h4><h5 id=141-人工干预>1.4.1 人工干预<a hidden class=anchor aria-hidden=true href=#141-人工干预>#</a></h5><p>切换的方法有很多种，其中最简单的方法就是人工干预，当 A 节点发生故障时，我们手动将请求切换到 B 节点。这种方法的缺点是服务无法使用，直到人工干预完成。这种方法看起来很蠢，但是如果不是使用在故障处理上，还是有可取之处的。</p><p>例如 MySQL 集群的迁移就可以用到这种方式；首先将新的节点加入最终一致性节点集合 E，也就是作为 Slave 节点来同步 Master 节点的数据。待到数据同步完成，ER 操作实际上已经可以切换到新节点上了；最后手动在新节点中启用一个 Master 节点，替换旧 Master 节点，然后将 CR 请求切换到新 Master 节点即可。</p><p>显然最后切换 Master 节点时与 C<sub>0</sub> 节点故障的处理等效，因此集群无法提供服务。</p><h5 id=142-哨兵机制>1.4.2 哨兵机制<a hidden class=anchor aria-hidden=true href=#142-哨兵机制>#</a></h5><p>为了自动化完成前面提到的人工干预方法，引入了哨兵机制。其本质上跟人工干预相同，不过是使用一个服务来替代人完成这一操作。优点是无需在原有服务上做修改，哨兵服务与原服务独立运行。</p><p>显然，哨兵服务的可用性也需要用集群来保证，因此 Redis 哨兵模式架构如下所示：</p><pre tabindex=0><code class=language-mermaid data-lang=mermaid>graph TD
    subgraph Sentinel
       A1(Sentinel Leader) &lt;--&gt; A2(Sentinel 1) 
       A3(Sentinel 2) &lt;--&gt; A2
       A3(Sentinel 2) &lt;--&gt; A1
    end
    subgraph Redis
        B(Master) &lt;--&gt; C(Slave 1)
        B &lt;--&gt; D(Slave 2)
    end
    Sentinel --&gt; Redis
</code></pre><p>哨兵服务监控所有 Redis 节点的状态，以便当 Master 节点宕机后进行自动切换。这样就解决了 Redis Master 节点宕机后新 Matser 的切换问题。</p><p>但是和 Redis 集群一样，哨兵服务在执行各种操作时，执行操作的主体只能是一个节点（Leader），其他节点作为备份（Follower），以达成一定的一致性和可用性。那么，当 Sentinel Leader 挂掉后，该如何切换新的 Leader 节点呢？如果还是使用哨兵机制的话，那么需要启用 Sentinel 的 Sentinel 服务，最终会无限套娃下去。</p><p>为了打破这一套娃循环，我们需要集群本身具有 C<sub>0</sub> 节点的切换机制，不能依赖外部服务。</p><h5 id=143-分布式共识算法>1.4.3 分布式共识算法<a hidden class=anchor aria-hidden=true href=#143-分布式共识算法>#</a></h5><p>前面在描述哨兵集群中的节点时，使用的是 Leader/Follower 而不是 Master/Slave，这是因为哨兵集群使用 Raft 算法实现了 C<sub>0</sub> 的自动切换。通常我们将使用选举算法实现的 C<sub>0</sub> 称为 Leader，实际上集群视角来看都为主从模式，只是“主”的故障迁移使用了不同的机制来实现；避免使用 Master 和 Slave 等词汇也有政治正确的体现（例如 GitHub 将主分支默认名称 Master 更改为 Main)。</p><p>首先要说明，此处不讨论<a href=https://zh.wikipedia.org/zh-hans/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E9%97%AE%E9%A2%98>拜占庭将军问题</a>，因为我们认为对集群中节点具有完全的控制权，因此集群内部的节点均按照规定的机制运行，出现的所有错误情况均是可预测的。</p><blockquote><p>因此这里讨论的分布式共识没有 BTC 等区块链项目的共识机制严格，但仍需考虑各种异常情况的处理；在这一点也能窥见区块链的魅力，在完全混沌和不可信的环境中构建一份共识。</p></blockquote><ul><li>Paxos 算法</li></ul><p>提到分布式共识算法就得从 <a href=https://zhuanlan.zhihu.com/p/341122718>Paxos 算法</a> 说起。Paxos 算法模拟了一个小岛上通过决议的流程，一个值的确定需要多数人发起提案（Prepare 阶段）后，得到多数人的同意（Accept 阶段）。其中每个人都要维护一个Proposal ID，已确保只处理自身视角里最新的 Proposal，Proposal ID 将会在 Prepare 和 Accept 阶段中更新。</p><p>这样的 Basic Paxos 算法的问题在于，只要多数人就可发起提案，因此很可能前一个提案还未处理完便发起了新的提案，导致 Accept 阶段有些人得知了新的提案便不再处理旧提案，使得旧提案无法得到多数人的确认，此时若再次发起提案，那么刚刚的新提案也会遇到同样的问题，造成活锁；同时，这样的流程仅能确定一个值，无法满足实际需求。</p><p>为了解决以上 Basic Paxos 算法的缺陷，很自然的提出了 Multi-Paxos 算法。为了解决只能确定一个值的问题，我们对每一个需要决定的值都使用一次 Paxos 算法；为了解决活锁的问题，我们限制提案只能由一个人发起，这个人叫做 Leader，代价是损失了一定的可用性，一旦 Leader 宕机算法将无法正常进行。</p><p>同时，由于 Leader 只有一个且算法将多次进行，那么在一开始提前使用 Paxos 算法确定好 Leader 后，后续的 Prepare 阶段都可以省略掉了；当 Leader 宕机后使用 Paxos 算法再选一个出来就可以了。这样还解决了算法 Prepare 阶段网络 IO 的耗时，提高了算法的效率。</p><blockquote><p>Leader 的选举还是使用的 Basic Paxos 算法，但是由于目的明确且易于控制，不会连续产生多个 Proposal 导致活锁。</p></blockquote><ul><li>Raft 算法</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://xinrea.cn/tags/distribute-system/>Distribute System</a></li><li><a href=https://xinrea.cn/tags/design/>Design</a></li><li><a href=https://xinrea.cn/tags/paxos/>Paxos</a></li><li><a href=https://xinrea.cn/tags/raft/>Raft</a></li><li><a href=https://xinrea.cn/tags/cap/>CAP</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://xinrea.cn>杏仁儿的博客</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>