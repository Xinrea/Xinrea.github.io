[{"content":"前言 为了降低应用程序开发人员的心智负担，数据系统提供事务这一机制来简化各种各样的常见问题。事务将应用程序的多个读写操作捆绑在一起，称为一个逻辑操作单元。\n执行事务只有两种结果，要么成功要么失败，且失败无副作用，不会产生任何修改；因此，如果失败，那么应用程序可以放心重试。有了事务，应用程序可以不用考虑某些内部潜在的错误以及复杂的并发性问题，这些都由数据库自己来处理，因此又被称之为安全性保证。\n一、理解事务 我们知道事务是数据库提供的安全性保证，用于屏蔽数据库内内部的错误以及并发问题，那么这个安全性保证具体包含哪些方面呢？这便是我们常说的 ACID。\n1.1 ACID ACID，即 Atomicity、Consistency、Isolation 和 Durability。显然，各个数据库能提供的安全性保证是不一样的，因此 ACID 在各个数据库上的表现也会有所区别，但是还是有比较笼统的概念的。\n1.1.1 原子性 Atomicity 想象这样一个场景，客户端发起了包含多个写操作的请求，而这个请求执行到一半时因为各种原因失败了（数据库错误/执行时判断失败），此时客户端该如何继续变成了问题；为了让客户端不用管理请求的执行进度，我们将其包装为一个整体的“原子操作”：如果执行失败，那么数据库必须丢弃或者撤销那些局部完成的修改。\n通过原子性保证，如果事务中止，应用程序可以确定没有实质发生任何更改，所以可以安全地重试（因此又被称为可中止性）。\n1.1.2 一致性 Consistency ACID 中的一致性指的是数据层面状态的一致性，例如数据库中 A 的值一定是 B 的值的两倍，那么从满足该一致性的状态开始，执行一个合法的事务后，得到的结果也应满足这个约束。显然，数据之中的约束是应用层定义的，数据库通常只能提供有限的约束工具，例如外键约束和唯一性约束，因此一致性通常是由应用程序本身来维护的。\n1.1.3 隔离性 Isolation 当多个客户端同时使用数据库时，如果访问同一记录，那么便会产生并发问题。为了让应用程序无需考虑并发竞争，数据库提供隔离性保证，这意味着应用执行事务时，从其视角来看所有的事务都是按照顺序在执行的，而数据库也要使用各种机制，确保这些事务在执行完成后所得的结果与顺序执行结果一致，这样才能符合应用的预期；实际中由于性能问题，数据库很少能够提供这样严格的隔离性，因此各个数据库提供的隔离性可能不同。\n1.1.4 持久性 Durability 持久性是大部分数据存储系统的基础保证，即数据写入（事务提交）完成后，不管发生什么情况，写入的数据都不应丢失。显然，持久性也是有限，不同数据库所提供的持久性保证都不一样：分布式数据库与单机数据库相比，提供的持久性通常会更好。\n综上所述，原子性保证对于各类数据库来说比较统一；一致性保证与数据库提供的工具有关，且责任在应用层；隔离性保证太过绝对，在各类数据库中都不太一样；持久性保证也很模糊，各类数据库或者说实际情况下都不一样。总而言之，也就原子性比较统一，其他概念在实际实现中都是比较模糊的。\n1.2 单对象写入与多对象事务 在前文提到了 ACID 的安全性保证，在数据库设计中，通常只用关心原子性和隔离性。对于持久性，这是存储系统的基石，无需额外讨论；对于一致性，如果应用提供正确的操作，而数据库保证操作的原子性和隔离性，那么便能保证一致性，因此也不单独讨论，因此我们主要聚焦原子性与隔离性在数据库中的设计和实现。\n接下来我们简单思考一下事务的一些注意事项。\n1.2.1 单对象写入 我们知道，最底层的原子性保证来源于 CPU 的设计，且其只保证了极其有限的操作；对于数据库存储的对象来说，那就更不存在原子性了，因此就算是单对象，其原子性和隔离性也不是天然的：例如数据库写入 100KB 的单对象时，在写的中途也要保证不被读到部分更新的数据（隔离性）；在写的中途失败时，写造成的修改也要能够撤销（原子性）。如果没有单对象的原子性和隔离性保证，那么就更别谈多对象的处理了。\n因此存储引擎在设计时便考虑了单对象的原子性和隔离性的支持，例如使用日志来实现原子性（可恢复可撤销），而使用锁来实现对象的隔离性。因此在我们通常的讨论中，单对象的原子性和隔离性太过基础，我们默认其是底层提供的基础保证，事务（多操作聚合）是在此基础之上的讨论，这也是为什么这部分标题并未称为“单对象事务”。\n1.2.2 多对象事务 在分布式数据库中，多对象事务还是有一定复杂度的，原因在于这些对象可能由于分区存在于不同的子数据库实例中。多对象事务对于一致性的保证有重要的帮助作用，例如同步更新的二级索引、外键引用的有效性、多文档同时更新等。\n1.2.3 处理错误与中止 前文我们提到事务提供了失败之后可以安全重试的保障，因此对于错误和中止，我们尽量尝试重试，但重试机制还有许多问题需要注意：\n事务执行成功，但应用认为执行失败（各种原因导致未收到成功确认），此时产生的重试可能出现问题：可以提供事务 id 以确保已提交的事务不会重复执行； 外因中止，例如系统超负荷运行导致事务执行失败，反复重试反而会加重系统负担：设定重试次数上限； 事务本身操作违反约束导致的失败：应用应调整操作内容而不是继续重试； 事务只能保证数据库内无副作用，如果有机制与数据库外操作相关，重试可能触发数据库外的副作用，例如更新时发送邮件，重试可能导致多次发送：涉及到多系统事务，可以使用两阶段提交； 本应进行重试的应用失效，导致重试操作丢失，可能导致数据丢失：应用也要具有持久化的能力。 二、弱隔离级别 接下来我们探讨事务的隔离性，由于这一部分比较复杂，因此单独进行说明。\n前文提到隔离性是为了解决数据库的并发访问问题而抽象出的概念，严格的隔离性使得并发操作执行的结果，与其按照顺序执行的结果一致，这意味着在应用执行事务时，无需关心并发的问题。然而在实际中，这种串行化的隔离性严重限制了数据库的性能，因此通常数据库提供较弱一些的隔离性保证，或是提供一些可选的隔离性配置。\n在非串行化隔离下，总会出现各种各样的问题，我们将问题归纳为以下几种进行标记，并讨论各种隔离级别对这些常见问题的解决：\nA脏读 B脏写 C读倾斜 D写倾斜 E幻读 F更新丢失 2.1 读-提交 读-提交通常是最基础的隔离级别，它只提供两个保证：\n解决 A脏读：读数据库时，只能看到已成功提交的数据； 解决 B脏写：写数据库时，只会覆盖已成功提交的数据。 关于为什么要防止脏读，原因很明显就不再赘述；我们重点关注脏写的问题：\n当有两个事务同时在执行时，前一个事务的写还未提交，而后一个事务想要写数据，如果在未提交的数据上覆盖，则称为脏写，最简单的处理方式是等待直到待写的数据被提交再进行。\n脏写会带来什么问题呢，想象这样一个场景，用户提交的事务需要修改 A、B 两处数据，同时有两个用户提交了事务在执行：\n最终导致 A 为用户 2 的数据，B 为用户 1 的数据；脏写会导致不同事务的并发写入最终混杂在一起。\n2.1.1 读-提交的实现 数据库要实现读-提交，那么就要解决 A脏读 和 B脏写。\nA脏读 的解决 一种方式是直接加读写锁，一旦数据正在被写入，则排斥读操作，但是这会导致只读事务的执行受阻，严重影响只读事务的响应延迟；因此大多数数据库使用另一种方式来实现：对于每个待更新的对象，数据库都会维护其旧值和当前持锁事务将要设置的新值两个版本，在这个事务提交之前，其他事务读取到的都是旧值，也就防止了 A脏读。\nB脏写 的解决 显然对于并发写，最简单的方法便是加写锁，数据库通常采用行级锁来防止 B脏写。\n2.2 快照级别隔离与可重复读 显然读-提交级别还有解决不了的问题，那便是 C读倾斜。考虑以下场景，用户正在使用事务 T1 查询两个账户的总余额，而后台有个程序正在用事务 T2 执行两个账户之间的转账：\nT2 更新了两个账户的余额，完成转账操作，两个账户的总额不变仍是 200；用户在查询 Account1 的余额时，由于 T2 还未提交，因此读取到的是读-提交保证的旧值 100，当其查询 Account2 的余额时，由于 T2 已经提交，因此读取到的是新值 200，符合读-提交的隔离性保证，但因此得出的总余额结果却是 300，这便是 C读倾斜，又可称为 C不可重复度读。\nC读倾斜 产生的原因是同一个事务内读取到了两个版本的数据，旧的 Account1 和新的 Account2，倘若 T1 在最后添加一个 Account1 的余额查询操作，便会发现前后两次查询操作结果不一致，这便是 C不可重复度读 这个名称的由来。\n在以下场景中 C读倾斜 很有可能发生：\n备份场景：在备份过程中数据库数据在不断更新，备份的数据可能新旧混杂，一致性被破坏； 分析查询与完整性检查：由于数据在不断更新，这类查询和检查无法实现。 C读倾斜 的解决 显然，事务在执行过程中应当读取一致的数据（同一个版本的数据），这为我们指明了版本控制这一思路。在读-提交中，为了解决 A脏读，我们维护了数据的新旧值，当写事务为提交时读旧值，提交后读新值；为了解决 C读倾斜，我们要更进一步，就算写事务已经提交，其旧值甚至更旧的值仍可能需要维护：有之前正在执行的事务需要读这个数据。\n这意味着旧值不再是一个临时维护的值，而是跟新值一样，是长期需要管理的值；考虑到多个正在进行的事务可能会在不同的时间点查看数据库状态，所以数据库保留了对象多个不同的提交版本，这种技术因此也被称为多版本并发控制（Multi-Version Concurrency Control，MVCC）。\n事务执行在数据库完整的一个版本快照上，因此又被称之为快照级别隔离。当事务执行时，首先赋予一个唯一的、单调递增的事务 ID。每当事务向数据库内写入新内容是，所写的数据都会被标记写入者的事务 ID。\n那么对于一个事务 Tk，其能访问的数据是哪些版本呢？考虑下图的场景：\n对于 事务 Tk 开始时：\n还未提交的事务 Ta：显然这部分数据不可见； 中止的事务：由原子性保证这部分数据不可见； 晚于 Tk 的事务：显然不可见； 其他所有写入都应可见。 简单来说，只有事务开始时，数据库内已提交的事务才可见；事务开始后数据的可见性不再变化，形成了“快照”。\n多版本看起来并不复杂，但是要考虑其与数据库其他功能的兼容性，最重要的便是索引与快照级别隔离兼容的处理。\n索引与快照级别隔离 显然，在多版本数据库中，索引指向的数据有多个版本，最简单的方法便是索引指向所有版本，找出后再根据版本进行过滤；对于使用 B-Tree 的数据库，还可以使用写时复制的技术：\n如图所示，在 B-Tree 中想要修改 D，写入新数据 D\u0026rsquo;，那么要修改其对应父节点中存储的指针，因此父节点也创建了新版本，同理一直往上，直到创建了新版本的根节点 R\u0026rsquo;。其中不受影响的 B、C 节点及其子节点，在两个树中被共享。\n在每个写入事务开始时创建一个新的版本树，然后在整个事务流程过程中都使用这个新树的根节点开始查找，无论是数据还是索引都是如此；在这个树中所能见到的数据都是这个事务所应该能看到的数据，因此无需进行版本过滤；显然，这些根节点就形成一系列快照。\n2.3 防止 F更新丢失 就算是快照级别隔离也存在问题没有解决，F更新丢失便是其中一个。F更新丢失发生在 read-modify-write 这样的操作流程里，这个流程十分常见，比如：\n计数器：一定是要读取当前值再进行修改 修改复杂对象的一部分 广义来说，F更新丢失也会出现在其他场景，比如 有关数据系统的一些简要笔记 中的写冲突部分，当两个用户并发写同一个位置的数据时，便会形成写冲突，某些解决方案会最终采用定义上最新的数据，造成F更新丢失。\n原子写操作 读-修改-写产生的问题不是新问题，此事在经典多线程程序中亦有记载，可以使用原子操作来解决。在多数关系数据库中，简单的 UPDATE 都是并发安全的，是数据库提供的原子更新操作。要实现原子操作，通常采取对读取对象加独占锁的方式来实现，这种技术有时被称为游标稳定性；另一种方式是强制所有的原子操作都在指定的单线程上执行，根据使用场景，这种方式的效率也并不低。\n显式加锁 由于原子操作提供的保证有限，无法适应一些应用层逻辑上的安全保证，因此数据库都提供了常见的锁机制。例如：SELECT \u0026hellip; FOR UPDATE，将满足指定条件的行都选出来，标记为 FOR UPDATE，即加上锁。\n如果数据库没有提供前文提到的原子写操作，用锁也可以很容易自己实现。\n自动检测更新丢失 前文提到的两种方式都是把操作串行化来避免F更新丢失，是一种悲观的策略；实际上，操作并行化，并非一定会出现F更新丢失，例如一个操作在读-修改-写，另一个操作在读-修改-写到别的位置，实际上并未冲突。因此可以采取一些乐观的策略，比如自动检测更新丢失。\n借助于多版本，我们可以很简单的检测出F更新丢失：当事务在读-修改-写时，最后写回时发现值的版本更新了，那么说明遇到了冲突，如果写入便会发生F更新丢失；当检测到后，可以回退事务，并使用原子的更新操作来重新执行。\n原子比较和设置 熟悉多线程编程便会知道，CAS 原子指令很有用，其实可以看成是弱化版的自动F更新丢失检测。自动更新丢失检测是使用版本号来判断，而 CAS 是根据值来判断。\n如果数据库提供了原子性的 CAS 指令，要谨慎使用，原因是多版本技术下，CAS 不一定能检测到版本变化，要注意其适用范围。\n冲突解决与复制 前文提到的方式实际上都是避免冲突发生，从而避免更新丢失的，然而在多副本数据库中，需要加锁的部分可能横跨多个节点，锁的实现需要考虑很多额外的东西；在多主或者无主系统中，甚至无法使用锁，因此只能在冲突发生后来进行解决，可以参照有关数据系统的一些简要笔记写冲突部分。\n2.4 D写倾斜与E幻读 2.5 串行化 严格串行执行事务 两阶段加锁 2PL 可串行化的快照隔离 ","permalink":"https://xinrea.cn/posts/transaction-in-data-system/","summary":"\u003ch2 id=\"前言\"\u003e前言\u003c/h2\u003e\n\u003cp\u003e为了降低应用程序开发人员的心智负担，数据系统提供\u003cstrong\u003e事务\u003c/strong\u003e这一机制来简化各种各样的常见问题。事务将应用程序的多个读写操作捆绑在一起，称为一个逻辑操作单元。\u003c/p\u003e","title":"数据系统事务的简单探讨"},{"content":"前言 大多数应用程序是通过一层一层叠加数据模型来构建的，如果某一层使用到了大量的数据，那么就需要一个数据系统来进行管理。\n怎么理解上面这句话呢，在一整个应用系统中，一条数据可能由不同的数据模型来表示，例如：\n在前端，数据以 JSON Object 的形式存在于内存之中，而其可能来源于 API 服务提供的 JSON 格式的一串字符； 这个 API 服务可能是用 Go 实现的，这条数据在构建为 JSON 字符串之前，可能在内存中以 Go Struct 的形式存在； API 服务可能是从 Redis 的响应中获取的数据，因此这条数据可能以 RESP（Redis 通信协议）规定的格式存在过，从 Redis 发送到 API 服务； Redis 能提供这条数据，说明这条数据在 Redis 的内存中以某种数据结构存在过； Redis 可能会将数据备份到硬盘，因此这条数据可能在硬盘上以某种格式存在过； 硬盘上，这条数据以某种磁场、电流的形式存在过； 那么为什么会存在这么多数据模型的转换呢？🧐\n因为每一层的需求都不一样。通常来说，数据在内存中的数据模型较为直接，但是也需要根据内存的特性进行优化（对齐）；在硬盘中，可能要考虑空间利用率；在服务与服务的通讯中（协议），可能要考虑编解码性能、数据大小甚至是可阅读性；在 Redis/MySQL 等数据库中，要考虑数据的检索查找性能。\n也就是说，在日常的编程过程中，无时无刻不在跟数据模型打交道，但是得益于前人的优秀设计与各种框架（各种语言内存模型与通信协议的规定），以及自身经验与最佳实践的指导，可能在无意间就选择了较优的形式来进行实现，通常情况不需要进行较为底层的数据模型的设计与选择。\n但是一些应用的功能就是保存、处理和检索大量数据，例如上文提到的 Redis、MySQL 等数据库，如果想要满足需求（通用、性能与数据安全等），则需要对数据模型进行仔细地审查与设计。\n针对这样的数据密集型应用系统，前人们有了许多的求索与探讨。\n显然，你可以为每一种数据单独设计数据模型进行优化，但这是不现实的，因为数据格式多样多变，数据系统作为应用系统的底层，承载着各种各样的数据，无法对每种数据进行专门的优化，但是对某一类数据进行优化是可能的，这个类可大可小，表现了不同程度的通用性。\n一、目标：通用 通用意味着我们需要从数据中总结出一套模式，还需要设计从这套模式中进行查询的方法。\n1.1 通用模型设计 1.1.1 层次模型 层次模型将所有数据表示为嵌套在记录中的记录（树）。显然，这种结构天然很好的支持了一对多的关系，但是对于多对一和多对多的关系，则没有明确的设计，因此实际使用时，要么将数据复制多份，要么留下一个引用进行手动处理（无法自动联结）；这些方式都会导致数据维护的困难。\n围绕多对多关系的处理困境，出现了关系模型与网络模型。\n1.1.2 网络模型 网络模型又被称为 CODASYL 模型，是层次模型的推广。层次模型作为树结构，每个节点最多只会有一个父节点；网络模型为了更好的支持多对多关系，将其推广，每个节点允许拥有多个父节点。每次插入新节点时，相关的父节点就进行了更新，天然进行了联结，形成了新的关系结构。\n为了实现这种灵活的数据关系，各个节点之间的关系变为了引用（指针）；由于过于灵活，数据访问的唯一方法是选择一条始于根目录的路径，并沿着相关链接依次访问，而这条链条也被称为访问路径。如果想要从根节点开始查询某个数据，那么需要进行大量复杂的遍历，十分低效，因此在新增相关的一组数据时，需要记录访问路径以便于后续从这些数据上检索，这使得每次查询都需要手动设定访问路径以进行优化，否则几乎不可用。\n同时，在维护数据时更是十分困难，一个节点可能有多个父节点指向它，对该节点的操作会引起大量变动。\n1.1.3 关系模型与关系数据库 关系模型直接否定了层次模型的嵌套结构：关系（表）只是元组（行）的集合，因此关系模型能通过元组天然表示一对一的关系，通过扩展外键（联结）实现一对多、多对一与多对多。联结关系（外键）作为额外扩展功能，由关系数据库实现。在如此简洁的结构之下，查询优化器诞生了，它用于查询顺序以及索引使用的自动选择，相当于自动构建出一条优化过后的访问路径，这也是通用性的一种表现。同样的，通用也增添了复杂，由于查询操作多种多样，查询优化器变得十分庞大且复杂；但总体来说这种方式兼具了通用和性能，成为了当下的主流选择。\n1.1.4 文档模型与文档数据库 关系数据库中，一对一关系能够天然直接表示，但是常见的一对多关系稍微有点别扭。在表和元组的关系下，一对多需要对父进行复制或者联结（或者说在不对等的关系中，需要对少的一方进行复制或者联结）。因此层次模型因为能天然表示一对一和一对多关系，还是在一些领域有一定的优势的，特别是在本身就是嵌套结构的文档数据领域。\n为了保留嵌套结构的同时实现多对一和一对多，文档数据库参考了关系数据库的实现，采用了“外键”的概念，称之为“文档引用”；这种引用的联结需要额外的操作来实现，因为其并未在结构中实际存在。\n1.1.5 图状数据模型 前文提到的模型在面对海量的多对多关系时仍然会力不从心，但是这种场景在现实中却是常见的，因此图状数据模型作为针对这一类数据的优化产物出现了。图由顶点和边组成，能够天然的表示多对多关系，在模式上更加的自由。具体有属性图模型和三元存储模型等类型。\n1.1.6 关系数据库与文档数据库的对比 关系数据库的基础结构很简单（元组），而文档数据库的基础结构为树；在文档数据库中，一对多关系表现为整体，而在关系数据库中则被拆分为多个元组，因此文档数据库因其局部性而具有更好的性能。在实际使用中，关系模型更多的是对现实模型的拆解，展示出其通用性；如果实际数据更符合文档模型，那么使用文档数据库会更加简洁；如果实际数据关系更加复杂（大量的多对多），那么使用图数据库会更加简洁。\n文档模型的灵活优势：由于文档模型中数据结构通常十分灵活，因此文档数据库通常不限制数据的具体模式，又被称之为“无模式”，但显然模式是存在的，只是交给了应用层处理，被称之为读时模式；与之对应的关系数据库则限制写入时数据的模式，要求其符合规则，又被称之为写时模式。 数据局部性的优劣：文档模型与层次模型类似，基础结构为树，在读取时则以树为单位，因此相关联的数据会被同时读取，如果要访问文档内大部分内容的话，则具有较强的局部性优势；但如果只是读取或者修改文档中的一小部分的话，就有点得不偿失，表现为劣势。如果修改后文档大小发生变化（主要是文档增大后），那么文档树甚至不能覆盖保存回原位置，而是需要重新分配。 1.1.7 关系数据库与文档数据库的融合 最简单的例子便是关系数据库的元组中支持 XML、JSON 等文档数据结构。\n1.2 通用查询设计 数据库数据模型的设计十分重要，但是考虑到数据库的主要功能之一：查询，设计出一种通用的查询方法也是至关重要的。与一般的程序调用 lib 不同，数据库存储着大量不同的模式，为了在这些模式上实现通用的查询等操作，数据库的接口必须十分灵活，这促使了数据查询语言的出现。\n命令式：网络模型（CODASYL）在前文中提到，每次查询需要设定访问路径，这个设定便是通过命令式查询语言实现的，这要求用户需要自行组织具体的查询逻辑； 声明式：关系数据库所使用的声明式查询语言（SQL 或关系代数）只需制定所需的数据模式、条件以及转换规则，查询优化器会组织具体的查询逻辑步骤，这意味着只要不断优化查询优化器，无需对查询语句修改即可实现性能提升。而且随着多核处理器的出现以及大尺度上集群的出现，声明式更适合查询优化器生成并行查询的逻辑操作，大大提高查询效率，可以说是适应了时代的潮流。 MapReduce：是一种混合命令与声明的查询方式，通过将命令式的纯函数代码片段复用，适应了集群分布执行的环境，可以作为 SQL 等声明式查询语言具体执行时的基础操作。 图状数据模型中，声明式和命令式查询语言都有。\n1.3 模型的实现 数据库，即采用了这些数据模型的数据系统，主要功能就是存储和查询。由于数据库作为专门存储和检索数据的系统，常常存储着远超系统内存大小的数据，为了同时管理内存和硬盘中的数据，引入了存储引擎的概念；就算以内存数据库著称的 Redis 等系统，为了数据安全通常也会启用持久化备份，同样需要管理内存和硬盘中的数据。\n数据库的核心，或者说存储引擎的核心是数据结构，我们以最简单的 kv 数据库为例，一步步探讨存储引擎的设计。\n假设 kv 数据按照更新顺序，依次写入文件尾部，那么在查找某个 key 时，我们从文件尾部向前扫描，找到的第一个值即为最新的值，复杂度显然为 $$O(n)$$我们把这个硬盘文件称之为段文件，显然对于拥有大量数据的数据库来说，这是不可接受的；因此还必须引入索引的概念，即帮助定位数据的元数据。显然写入文件尾部这一操作已经是数据写入的最简操作了，任何额外的元数据构建或者说索引的引入，都会降低写入性能。在这一简易场景下，最简单的索引便是 HashMap，也称之为哈希索引。在这里我们拥有了追加写段文件与哈希索引，分别存在于硬盘和内存中，结合起来完成数据的管理。\n1.3.1 追加写段文件 + 哈希索引 追加写段文件在添加数据时具有极好的性能，因为其基础操作是最简单的往文件尾添加数据；虽然会有许多旧值存在，但是通过扫描段文件能很轻易地实现段文件的整理和压缩，保留每个 key 最新的值，只需在后台定期压缩即可。考虑到单文件过大带来的问题，追加写段文件能很轻易地在文件大小达到阈值后进行分段，只需要写入新文件即可；对于这些段文件的压缩和合并可以定时在后台完成。\n在内存的 HashMap 中记录硬盘上每个 key 的偏移值，这样查询能在 O(1) 完成；这个 HashMap 能够通过扫描段文件构建，或者是构建完成后保存在硬盘中，重启后读取构建。如果进行了分段，那么每一段都有其对应的 HashMap，在查找时只需要根据段的新旧顺序，从新往旧搜索即可。\n两者的配合可以实现数据量较小情况下的高性能数据库。但由于哈希索引的设计问题，导致\n系统内存需要能够容纳所有的 key; 由于段文件无序，区间查询时只能通过哈希索引在段文件中跳来跳去读取，拼凑出区间结果，效率很低。 1.3.2 有序段文件 SSTABLE + 哈希索引 为了解决前文提到的问题，很容易想到将段文件设计为有序的，相对应的有两大好处：\n系统内存中只需记录部分 key 的位置，当查询某个 key 时，只要能找到其可能所在的区间就可以了； 段内数据有序，可以按顺序直接读出区间值。 显然，不能只规定说段文件有序就行了，需要思考 🤔 如何维护其有序的状态：\n显然不能直接往段文件尾部追加写了，这样无法维持其有序的状态，所以必须在内存中缓存一部分的数据，我们称之为内存表，当其达到一定的量后再排序写入段文件（或者从一开始就使用有序数据结构来管理，例如红黑树，避免写前排序），这样产生的段文件就都是有序的了； 如果系统崩溃，那么内存中缓存的部分数据会丢失，因此还是要将其持久化到硬盘，以便于恢复；显然，可以使用前文中追加写段文件的形式来持久化这部分数据，由于我们已经规定段文件有序，那么这个“追加写段文件”我们便另外称之为“追加写日志”，不参与查询和合并操作，只用于故障恢复； 对于段文件的压缩，有序后反而更简单了，不必将两个段文件完全读入内存便可以完成合并压缩工作，因此这部分不受影响； 还有一点细节，当内存数据写入有序段文件后，这部分的追加写日志便失去了作用，可以丢弃了。 要注意的是，上述过程中所有写入硬盘的操作都是顺序写，因此写性能极高。\n这部分所提到的段文件的维护方式，正是 LevelDB 和 RocksDB 所使用的；而前文提到的直接追加写的方式，则是 Bitcask 所使用的。\n基于合并和压缩排序文件原理的存储引擎通常被称为 LSM 存储引擎；其中 LSM 表示 Log-Structured Merge，原因是这些有序段文件实际上是追加写的日志文件演变而来，且基于这些文件的压缩合并。这个结构也被称为 LSM-Tree（日志结构的合并树），原因是实践过程中，有序段文件的合并并不是随意进行的，我们将最基础零碎的段文件不断合并后，会形成一系列经历过一次合并的段文件，而这些段文件还能进行合并，依此类推，是存在层级结构的，这样便形成了树；我们可以针对不同层级采取不同的压缩策略。\n1.3.3 B-Tree 前文提到的 LSM 存储引擎虽然简单好用，但是仍有其缺陷：\n其后台进行的合并压缩操作会占据一定的硬盘读写带宽，如果数据量巨大，合并压缩操作完全占据了磁盘的读写带宽，那么数据库便无法对外提供服务； 就算没有全部占据，合并压缩操作也会对服务请求的处理产生影响，造成随机的响应延迟，有点类似于带有 GC 机制的语言的缺点； 其次，不同的段文件中可能包含 key 新旧时期不同的值，为事务语义的实现增添了不少风险； 读取性能较弱，虽然用布隆过滤器可以避免许多不必要的查找操作，但是搜索范围可能涉及多个段文件的读取； 二级索引实现困难。 关于索引与存储引擎，显然存储引擎为了实现高效的查找功能，在存储设计时一定会有对应的索引设计，就如上文中的哈希索引一样，在 kv 数据库中表现为 key 的索引；在关系数据库中表现为主键的索引（必定有一个这样的索引）。在这个主索引之外，通常还需要创建二级索引，以加速非主键上的查询操作。\n有没有响应时间稳定，读取性能更好，二级索引简单的存储引擎选择呢 🤔？那就是老牌的 B-Tree 了。\nB-Tree 把数据库分解成固定大小的块或页，页是内部读写的最小单元，与磁盘的块排列设计相匹配；每个页面都可以用其在磁盘上的位置进行标识，相当于磁盘上的指针。其中一个页作为 B-Tree 的根，所有的查询操作都从这里开始。\n页中包含键和其他页的引用，每个引用代表着一个连续范围内的键，相邻引用之间的键可以指示这些范围的边界，这些数据都是有序存放的，便于二分查找。B-Tree 中一页所包含的引用数称为分支因子，实际中这个因子会收到页大小等因素的影响，会达到几百。\n在 B-Tree 中更新某个 key 时，首先要找到其值所在的叶子页，然后将其修改后覆盖整个叶子页；如果添加某个 key 时，其值应在的叶子页已满，那么就需要进行分裂操作。分裂操作意味着原来指向叶子页的引用也将变成两个，因此会涉及到父页、原叶子页与两个新叶子页的处理；如果删除某个 key 后仍然要使 B-Tree 保持平衡，那么需要更多额外的操作。\n平衡状态下，具有 n 个键的 B-Tree 总是具有 O(logn)的深度，具有优秀的查询性能。但如同上文介绍的 B-Tree 的更新、新建和删除操作中对页的操作，当涉及到多个页的更新，甚至是单页的覆盖时，磁盘无法同时完成这些操作，因此这些操作并非原子化，可能在操作中途中止，导致数据的一致性损坏。同 LSM-Tree 一样，可以使用 WAL（Write-Ahead Log）来记录相关操作，该日志用于将 B-Tree 恢复到最近一致的状态；同样的，多线程访问 B-Tree 时也会出现问题，因此要进行并发控制。\nB-Tree 的优化 由于 B-Tree 下操作的基本单位是页，而页在覆盖过程中出问题就导致数据损坏，因此可以使用写时覆盖方案：被修改的页并不原地覆盖，而是被写入别的位置，这样仅需修改父页的指针即可，我们可以在确认被修改的页写入完毕后最后才修改指针，因此这期间并不影响其他数据的访问。 页的大小是固定的，我们希望能尽量提高页空间的利用率，以保存更多的键和引用，提高树的分支因子，进而减少层数。由于节点中键是有序排列的，中间的键其实可以只使用缩略信息，并不影响键的查找。 相邻子页存放在相邻位置，但是会很难维护 额外指针连接相邻的叶子页 关于 B-Tree，可以学习 SQLite 的实现，简单易懂，有些课程甚至教你如何实现自己的 SQLite，例如：Build your own SQLite。\n1.3.4 数据仓库的优化 与我们常见的强调交互性的事务处理不同，还有一种数据系统是用来大量数据分析的，并不太强调交互性，提交的任务几个小时甚至几天处理完成都可以。针对这样的超大数据分析场景，也有一些常见的优化策略。\n典型的数据仓库中，表的列十分之宽，而且每次进行分析时，经常都是把整列所有的数据都拉出来进行处理，因此出现了列式存储和列压缩的概念：\n列式存储将一列的所有数据存放在一起，特别适合整列查询较多的场景； 使用列式存储后，便很容易使用列压缩来降低磁盘吞吐量的要求，原因是一列中，通常会有大量重复数据； 二、目标：可用 前文简单介绍了数据系统的一些底层通用设计，但是仅仅考虑这些单机底层的问题还不够，因为现实中数据太过珍贵，作为一个数据系统，无法容忍数据丢失的出现；因此在讨论完数据系统的底层细节后，还要从更宏观的角度来设计以保证其可用性。数据系统的可用性更是一个复杂的主题，原因是计算机硬件和网络的不稳定性带来了无法忽视的问题和挑战。\n“当一个不可能出错的事物出错了，通常也就意味着不可修复” ——Douglas Adams，《基本无害》（1992）\n这意味着要提升数据系统的可用性和健壮性，不能想当然的认为其某部分是不可能出错的，而是要慎重考虑各部分出错后的备案和处理方式。\n2.1 数据复制 当一个数据系统部署在单节点上后，出现的第一个问题便是：这个节点挂掉了怎么办 😨？ 很自然的便能想到“备份”这一方案；在多台机器上保存相同的数据副本，这便是复制，也便产生了分布式数据系统。\n你可以仅仅把备份当作是备份，以提高可用性：当节点挂掉后切换到备份节点提供服务；你也可以让备份平时也发挥点作用，对外提供服务，以提高整体的读吞吐量；你还可以精心挑选备份节点的物理位置，让它们为用户提供更低延迟的服务。\n在数据不会发生改变的场景下，上述讨论就已经很完美了，但是现实情况不一样，数据系统在不断的更新数据，这才是复制的挑战所在。\n显然，数据系统在不断更新数据的时候，我们要以某个或者某些节点的数据为准，其他节点“复制”这些节点的数据，因此天然区分出了主节点与从节点。按照主节点数量的不同，我们可以分出这几类复制方式：主从复制、多主节点复制和无主节点复制。\n2.1.1 主从复制 主从复制是从单节点扩展出备份节点后天然形成的复制模式，为了保证每个节点上的数据保持一致，我们可以想象，为了保证数据的一致，任何一个写入更改操作，无论经过了什么操作处理流程，最终终将在每个节点上执行。\n在主从复制中，主节点负责接受写操作，并将其转发给各个从节点，这样便让所有节点都能接受到相同的写操作；对于读操作而言，由于其并不会对数据进行更改，因此从主从节点上读都是可以的。\n这个转发的过程，也就是写操作何时在各个节点上执行，也有不同的策略选择，这便是同步复制和异步复制。从名称上就能看出，同步复制需要等待写操作在各个节点上执行完毕后，用户的写操作才算是完全结束，如果有任何一个节点失败（失去状态同步），那么操作就算是失败；异步复制则是主节点完成写操作，便视为用户的写操作完成，而从节点能否接收执行写操作，用户不管，由分布式数据系统来尽量保证这一点。\n可以想象，同步复制需要每个节点都完成操作，但只要有一个节点故障便会操作失败，当节点数量很多时，操作的失败率便指数级上升；异步复制只需考虑主节点，因此不管节点数量有多少，都不会影响异步复制情况下操作的执行。\n异步复制看起来和实际用起来性能却是很好，但是与同步复制相比缺少了一个保证：同步复制保证写操作完成后，所有节点的状态都是最新的，可以随便读；但是异步复制只能保证主节点是最新的，需要等待一段由无数因素影响的时间后，节点的状态才能都更新到最新。当然，聪明的你一定能想到，中庸之道，综合这两种方式，在系统中存在同时存在同步节点和异步节点，在同步和异步的优缺点之间 lerp 一下，找到合适自己的设定。\n要注意的是，在异步复制中，由于那个虚无缥缈的更新延迟时间的存在，主节点故障会导致数据丢失，这是在很多情况下都决不允许的，但是现代分布式数据系统对性能的要求很高，无法割舍异步复制带来的吞吐性能，因此只能在此基础上打各种补丁，尽量避免数据丢失的发生。\n接下来要考虑的是节点故障后该怎么处理，由于系统中只存在两类节点，因此我们分别进行探讨。\n从节点失效 从节点只是起到一个备份作用，而且从节点往往不止一个，所以如果只是从节点失效，那还是挺让人放心的，并不会影响系统的可用性；从节点只需要根据自身已持久化的数据被日志，对比主节点的日志，便能同步缺失的各种操作，一旦同步完毕便能够正常提供服务了。\n主节点失效 在主从复制中，主节点负责所有的写操作，一旦主节点失效，便意味着系统无法对外提供正常的服务了，因此主节点失效的处理至关重要。\n尽快发现失效：由于节点们可能位于不同的物理位置，又都是相对独立的在运行，因此节点是否失效，保底的检测机制只能是超时。而超时时间的设定也有很大的影响，如果设置过长，那么恢复时间也会变长；如果设置过短，很有可能导致节点误认为失效，出现频繁失效切换。 尽快切换主节点：原来的主节点挂掉了，我们又希望系统能尽快重新上线，那么不可能指望原主节点修复故障、重新启动，只能是立马决定换一个可用的节点作为主节点。这个切换的过程涉及到新主节点的选举，以及如何让其他节点知道主节点切换了；这些可以是问题也可以不是问题，比如，你可以手动选择节点，手动更新其它节点的配置，但显然这会引入最不可靠的“人”进入系统，因此需要一套称为共识的机制，来自动完成这一操作，关于共识会在后文深入探讨。 就算如此，主节点失效还是很危险的，原因是超时并不意味着原主节点真的停止了任何工作，异常情况下原主节点很有可能不知道自己出问题了，就像是精神病患者不认为自己有精神病一样，这样很可能出现双主节点同时工作产生冲突的情况，因此我们需要让系统内所有节点统一决定原主节点是否已经失效，如果一致认为原主节点失效，那么就算原主节点工作正常，也将被当作精神病处理，忽略它的任何请求，这个问题也等同于一个创建一个共识。\n复制没有完美的解决方案，只能在众多参数中，例如副本一致性、持久性、可用性以及延迟等，寻求最适合最平衡的那个点；这同样也是其它分布式系统问题解决方案的核心：取舍。\n异步复制的问题 异步复制由于其同步延迟的存在，就算节点都工作正常，也会有各种各样的问题，下面简单介绍几种常见问题：\n单调读一致性：当用户多次读取同一数据时，由于节点状态有新有旧，很有可能第一次读到新数据而第二次却读到了旧数据，这就不满足单调读一致性； 读写一致性：当用户写完数据尝试读取时，由于有的节点状态还没更新，可能读到旧的数据，导致用户观察到了“数据丢失”，这就不满足读写一致性； 解决方法也很直观，针对单调读一致性，需要让用户多次读取时都读取同一个节点，这个可以通过用户标识来唯一指定读节点来解决；针对读写一致性，可以让用户写后只从主节点读取数据；\n还有出现在分片情况下的特殊问题：前缀一致读。由于还未介绍分片，因此这里只简单介绍一下：不同用户的因果相关的两个操作被分在了两个分片上，那么它们被查询时，由于异步复制延迟的存在，并没有先后关系，导致因果关系混乱。\n使用最终一致性系统时，首先就要思考复制延迟增加到几分钟甚至几小时时造成的影响，如果不可接受那就需要采取措施提供一个更强的一致性保证。\n2.1.2 多主复制 由于主从复制中，主节点承担了太过重要的角色，一旦主节点失效，那么等同于系统失效，因此我们自然地进行扩展，想到将主节点配制成多个，分担写操作的工作。显然，系统内节点的状态还是要统一的，因此每个主节点也会接收其他主节点的写操作，就像是从节点一样。\n显然，多主节点的设计使得写操作的同步变得十分复杂，通常来说在一个数据中心内使用多节点没有太多意义，也并没有解决太多的问题，所以我们一般只在以下几种情况内使用多主复制：\n多数据中心：由于物理的限制，主节点可能与从节点距离非常远，造成通信消耗极大，整个系统的效率急剧降低； 离线客户端：网络断开后还需要能够正常使用，因此需要在本地配置一个主节点（本地数据库）来接受写操作，在联网时就相当于有多主的问题需要处理； 协作编辑：例如 Google Docs 等文档应用，都是允许多个用户同时进行编辑的，每个用户都有着像“主节点”一样的写权限，先修改本地，然后同步给其它节点。 显然，从上面列出的场景可以看出，多主复制的主要挑战是解决冲突（显然，多主复制的复杂情况下更不可能使用同步复制了，因此我们讨论都是基于异步复制）。\n写冲突 这是多主复制中最常见的情况，例如两个用户“同时”从两个不同的主节点更新了数据库中的某一条记录，当这两个主节点想要将修改同步时，便产生了冲突，究竟该怎么处理，或者说采用哪一条？\n避免冲突 在逻辑上这两条是完全平等的，我们需要制定规则，强行分出先后优先级。🤔 或者说，我们可以先发制人，解决提出问题的人：我们可以尽量避免冲突。\n写冲突产生的条件是两个主节点接收并处理了同一条记录的写操作，那么我们让特定数据的写操作只发生在指定主节点上，这样将数据的写操作划分分配给不同主节点后，便不会产生冲突了。\n可喜可贺，但是我们仍需考虑这种方案故障下的情况，例如用户想要写某条数据，但是他与指定的主节点网络连接不上，但是与其它主节点连接正常，难道我们要拒绝为用户提供服务吗？当然可以拒绝，但是可能更好的方案是 在这种特殊情况下我们可以让其它主节点接手一定的工作，唯一的代价是可能会有写冲突产生。\n收敛于一致状态 当然，除了数据写操作分工方案可能出现故障外，还有一些情形是这种方案无法使用的，例如前文提到的协作编辑、离线客户端等，它们的本地主节点是大概率编辑同一条数据的，写冲突是无法避免的。在这种情况下，我们希望 根据有限的信息进行写操作的前后人为排序，而且人为排序一定是稳定的，这意味着无论哪个节点遇到这样的冲突，拿到相同的信息后，做出的判断都是一致的，这被称为收敛于一致状态。\n下面举例几种收敛于一致状态的处理方式：\n以某种方式将这些冲突值合并或者保留，例如按照字母排序然后拼接在一起。本质是保留信息，交给用户或者是后续再处理； 给写入操作分配唯一 ID，根据唯一 ID 判断顺序，将最高优先级的操作写入，其它丢弃；显然会造成数据丢失； 给节点或者说副本分配唯一 ID，根据副本 ID 判断顺序，将高优先级的操作写入，其它丢弃；显然也会造成数据丢失。 这里的数据丢失可以这样理解：由于写冲突是不同的用户在修改相同的数据，那么如果用户的操作被丢弃，那么他想要修改的值甚至不会在数据库中出现，在他看来就好像他的操作丢失了一样（事实也确实是丢失了）。\n有人可能会疑问 🤔，那他在编辑记录里也看不到自己的操作吗？显然不能，因为他的写操作都未在数据库上执行，更无法保存在编辑记录中了。那有没有可能将冲突操作排序后，按照顺序都执行一次呢？这个想法很好，只可惜现实情况是：\n首先在冲突产生之前，这些操作已经在各自的节点上执行过一次了，如果想要按照顺序执行一遍，还需要撤销之前的操作； 其次由于异步复制，等收集完所有的冲突操作后再整体排完序是不可能的。同一个位置的冲突可能表现为，冲突解决后，过一段时间又在同一个位置收到了一个冲突的写操作（可能网络延迟导致晚点了），需要再次与保存的写操作进行冲突解决。这也是要使用收敛于一致状态的处理方式的原因，每个节点解决冲突的顺序是不一样的，但是要保证解决完结果都保持一致。 在冲突处理上，实际上都是人为规定的解决方案，有人会说严格按照时间戳来判断不就行了，但是在分布式系统中各个节点的时间戳都是有偏差的，没有准确的全局时钟存在。因此各个方案并无高低之分，重要的是冲突解决的结果符合业务的需求。因此许多数据库提供自定义冲突解决的方案，当写入或者读取时执行冲突解决代码。\n前文提到的冲突是很直观的直接对同一条数据修改造成的冲突，实际上同时对不同的数据也可能造成冲突，这种冲突是逻辑上的，破坏了数据中隐含逻辑关系的。例如：两位员工都有一个休假标识，规定这两位不能同时处于休假状态；某天他们同时申请休假，按照程序都检查了另一位的休假状态，于是放心地在不同的主节点上执行了休假操作，操作也没有任何冲突产生，但是结果就是二人都处于了休假状态。\n自动冲突解决 前文提到了数据库提供了自定义冲突解决方案，只需提供冲突解决代码即可；但是这个代码可不是那么容易就能写好的，因此有前人研究了一些比较完善的适合冲突解决的数据结构或者算法，给出了更加严谨和通用的解决方案：\n无冲突的复制数据类型（Conflict-free Replicated Datatypes，CRDT） 可合并的持久数据结构（Mergeable persistent data） 操作转换（Operational transformation） 感兴趣可以自行深入了解。\n拓扑结构 前文有提到多主复制架构中，主节点还要接收其它主节点发来的写操作，说明这些主节点间存在着可能的多种联系路径，这便是多主复制主节点的拓扑结构。\n很容易想到，如果某个主节点只能从另一个特定的主节点到达，那么一旦那个主节点出现故障，这个主节点便无法完成状态同步了。按照节点间访问路径的多少，大致可以分为以下三种常见的拓扑结构：\n环形拓扑 星形拓扑 全链接拓扑 显然访问路径越多，对于节点的故障容忍能力就越好；但是当访问路径有很多时，路径速度有快有慢，很有可能后发生的操作会先于之前的操作到达，如果这两个操作间有因果关系，那么便会产生错误。为了让操作消息正确有序，可以使用版本向量，这将在后文详细讲解。\n2.1.3 无主复制 说是无主，实际上是全主，意味着所有副本都可以接受来自客户端的写请求，我们目前把无主数据库称之为 Dynamo 风格数据库。每次进行写操作时，要么客户端直接将请求发送给多个副本，要么客户端将请求发送给协调者节点，由协调者决定发送给哪些副本。\n显然，当客户端可以将写请求发送给每一个副本并完成时，这个数据库能提供其正常的服务；但是当客户端只能发送给部分副本，或者说部分副本失效时，该如何处理呢？当然，这里的处理分为两个部分，一是某些副本数据较旧，读到旧数据怎么办；二是数据较旧的副本，该如何保持状态同步。\n副本状态不一致读问题：为了解决这一问题，我们可以想到，客户端既然可以将写请求发送给各个副本，那么读请求也不必拘泥于某个副本，而是并行的读，然后取最新的一个值；显然，这需要每个值都有一个版本号。 旧副本状态同步：从问题 1 的解决方案可以看出，在并行读的时候是能够察觉到某些节点的状态落后的，因此可以使用读修复，在并行读后更新那些落后的副本；如果读操作很少，那么就不能只依靠客户端的读操作来同步了，我们可以在后台运行一些进程专门用于对比缺少的数据来进行复制，这被称为反熵过程。 读写 quorum 前文提到客户端自己或者通过协调者将写请求发送到多个副本，那么这些写请求是否成功该如何判断呢？假设需要所有副本都成功，那么显然回到了同步复制的问题，一旦节点数量达到一定数量，写操作就很难成功了；从前文我们还知道，部分副本就算没有更新到最新的状态，我们通过并发读还是能获取正确的数据，这就意味着并发读使得写操作有了一定的容错。\n推广到一般情况：如果有 n 个副本，写入需要 w 个节点确认，读取必须至少查询 r 个节点，则只要 w+r\u0026gt;n，读取的节点中一定会包含最新值。满足这些 r、w 值的读写操作称为仲裁读和仲裁写，也被称为严格法定票数。\n为什么被称之为“严格”呢？我们知道并发读使得写操作有了一定的容错，而一般情况并没有限制读发生在哪些节点上；如果我们从 n 中取子集 n\u0026rsquo;，且确保读写都发生在 n\u0026rsquo; 内，那么只要满足 w\u0026rsquo;+r\u0026rsquo;\u0026gt;n\u0026rsquo;，还是能起到同样的效果的。\n反之，就算满足了 w+r\u0026gt;n，也不一定就能读到最新值：例如在一些边界条件下，两个写操作同时发生，导致其中一个写操作在冲突处理中被丢弃，读便会发现数据丢失；如果读写同时进行，那么写操作可能还未进行完，导致读去旧值。由于现实情况的复杂性，Dynamo 风格的数据库通常不追求一定能读取到最新值，而是针对最终一致性场景进行的优化，因此在配置参数 w、r 时，应当其作为概率保证而不是绝对保证。\nsloppy quorum 前文提到我们设定 n 或者其子集 n\u0026rsquo;，只要满足 w+r\u0026gt;n 都能够实现相同的效果，因此我们无需使用所有节点，我们总假设 n 为全部节点的某个确定的子集。但是由于读写操作是独立的，我们无法将其分组，只能规定在一定时间内，读写操作都在 n 节点上进行才能持续保证 w+r\u0026gt;n 的条件。\n实际情况是多变的，可能出现客户端无法访问全部 n 中节点的情况，是否为这样的客户端提供服务，便成为了一种选择。如果为这种情况提供服务，那么我们称之为宽松的 quorum：写入和读取仍然需要 w+r\u0026gt;n，但是写入和读取的节点不一定在 n 中；当网络恢复后，临时接受请求的节点需要把收到的写请求转发给 n 中的节点，这被称之为回传；在回传完成之前，读操作无法保证都能读取到新值。\n冲突处理 与多主复制类似，无主复制中也会产生写冲突，原因是一旦支持写并发，那么说明写之间的顺序是不确定的。与多主复制类似可以采用最后写入者获胜（last write wins, LWW）：\n按照一定的规则对并发写操作进行排序，然后保留最新的那一个，这被称之为最后写入者获胜，这里的“最后”不一定是时间上的最后，只是表达了一种顺序的概念；显然，同多主复制一样，也会导致数据丢失。要确保 LWW 安全无副作用的唯一方法是：只写入一次然后写入值视为不变，这样就避免了对同一个主键的并发（覆盖）写，实际中可以对每个写操作都使用其 UUID 作为主键，只是在查询时同一个 key 会查询到多个数据，需要进行处理。\n并发与并发检测 前文提到的冲突是发生在写并发的场景下，假如两个操作属于并发关系，才需要考虑冲突解决；否则，两个操作有先有后，后操作是可以安全覆盖前操作的；为此我们必须考虑并发检测。我们定义，如果两个操作不需要意识到对方，那么它们是并发操作。通过区分前后与并发关系，我们可以使用比 LWW 更好的方法，来避免数据丢失：\n每个主键都有一个写入时递增的版本号，值可能有多个最新版本，对应并发写操作； 写前必须要发起读，且读会提供所有最新版本的值； 写时必须提供之前读到的版本号；且写的响应可以与读一致，这样可以减少一次读操作，实现连续写； 数据库收到写操作时，由于其带有版本号，因此可以覆盖该版本号或更低版本的所有值。 这样通过版本控制，并发写不会导致数据丢失，但是多个版本的最新数据仍需考虑如何处理，这里可以提供自定义的解决方式，例如最常见的操作便是合并；也可以和多主复制一样，使用一些专门的数据结构来执行自动合并。\n版本矢量 前文提到每个主键都有一个写入时递增的版本号，在多副本的情况下，每个副本的同一个主键的版本号情况可能是不同的，因此我们收集所有副本的版本号信息，集合称之为版本矢量；通过版本矢量，来决定是否覆盖或者保留。\n2.2 数据分区 介绍完了数据复制，我们可以想象数据被复制成为了多个副本以保证数据的安全性和系统的可用性，但在实际中数据的量可能十分庞大，超出了单个节点的最大容量，或者单个节点无法承担这些数据的写入和读取请求，我们必须考虑把一份数据拆分存放，这便是数据分区，也就是前文提到的数据分片。\n在完成数据分区后，每个分区就可以运用前文提到的各种技术，进行数据复制了，分区之间具有一定的独立性，可以看作是完整的小型数据库；但是不可避免的，一些操作会同时涉及到多个数据分区，我们在后面会进行详细的探讨。\n2.2.1 键值数据的分区 面临海量的键值数据，该怎么进行分区呢？或者说分区的目标是什么呢？\n无论是数据量太大因此单节点无法承受还是因为读写操作单节点无法承受而进行分区，都是因单节点到达瓶颈产生的选择，因此在分区后我们希望尽量避免单节点瓶颈，这就要求分区的结果一定要均匀。\n基于关键字区间分区 我们规定关键字的顺序后，便能按照顺序手动划分成许多区间，然后按照这些区间进行数据分区，显然这种分区方式很灵活，因为区间设定可以手动也可以自动，但这要求管理员或者数据库对这类型的数据有一定了解，才能够划分均匀。\n基于关键字哈希值分区 通过使用一个好的哈希函数，可以让各个关键字哈希后在值域上均匀分布，此时在值域上进行分区便能比直接基于关键字区间分区更加均匀；但问题是哈希分区后，相邻的键被分配到了不同的分区，区间查询无法连续进行了。有些数据库如果启用了哈希分片，那么在区间查询时便把请求一次性发送到所有的分区；而有些数据库则干脆不支持关键字上的区间查询。\n为此，有些数据库采取折中的策略，可以将多列组成复合主键，并将较少执行区间查询的列用作哈希关键字进行分区，这样便不会影响常见的分区查询操作。\n负载倾斜与热点 数据的均匀并不意味着负载的均匀，原因是某些数据可能会被高频率访问；如果要解决热点问题，最简单的方法就是在热点关键字后加上随机数，这样关键字变动会使得这个关键字的数据被划分到不同分区，以减轻节点压力，这需要读取操作时同时读取这一批关键字合并才能得到结果。\n2.2.2 分区与二级索引 前文提到的键值数据分区，暗示讨论的情况是数据只有主键索引的情况；如果引入二级索引，情况会变得复杂起来。但是二级索引是十分重要的，在以查询业务为主的数据库中，对查询的加速离不开二级索引。因此我们以文档数据库这类以查询为主的数据库为例，探讨二级索引的分区。\n二级索引带来的主要挑战是它们不能规整地映射到分区中，我们要考虑二级索引如何在分区之中存放。有两种主要的方法来支持对二级索引进行分区：\n基于文档的二级索引分区 二级索引通常使用倒排索引实现，这意味着二级索引记录着其 key 出现在哪些文档中，具体表现为每个索引条目对应一个文档列表。我们在每个分区中独立为其包含的文档建立二级索引，这就意味着不同分区的二级索引是独立的，因此也被称为本地索引；当进行查询操作时，需要同时查询所有分区的二级索引，汇总后才能得到最终结果，这会导致显著的读延迟放大。\n基于词条的二级索引分区 我们可以对所有文档建立二级索引列表，然后对列表进行分区，存储在不同的节点上，这也被称之为全局索引；这意味着每个分区上存储的二级索引与那个分区上存储的文档没有什么关联。其优势是每个二级索引词条存储的都是完整的文档列表，只需查询一次即可；但文档更新时，由于其涉及到的二级索引位于各个分区，因此会引入显著的写放大。因此现有的数据库都不支持同步更新词条分区的二级索引。\n2.2.3 分区的再平衡 分区并不是一劳永逸的，随着时间的推移，平均总会变得不平均，因此分区的再平衡是在使用前就要考虑到的工作。\n考虑到数据量的庞大，我们希望分区再平衡时移动的数据越少越好。对此我们主要有以下三种处理方式：\n固定数量的分区 分区数量虽然固定，但是我们可以在一开始划分较多的分区，并且让一个节点负担多个分区；这样当节点压力过大时，我们不重新分区，而是把现有分区抽取部分转移到新节点上，以减轻节点的平均压力。显然，一开始决定的分区数量十分重要，过多的分区会导致额外管理开销过大；过少的分区会使得后续的再平衡困难。\n动态分区 当分区数据量增长超过阈值后，便会拆分为两个小分区；当分区数据量缩小到某个阈值以下，也可以合并成一个较大的分区。拆分后的小分区可以将其中一个交由别的节点进行管理。\n按节点比例分区 固定数量的分区和动态分区中，分区数量与节点数量没有任何关系，一个是固定数量而一个是按照策略分裂合并。对此，引入了节点数量作为参数的新方式，使分区数量与集群节点数成正比关系：\n当一个新节点加入集群时，它随机选择固定数量的分区进行分裂，然后拿走一半的数据进行管理。\n这个方案要求使用哈希分区，原因是如果是按照关键字分区，取走的区间很有可能都位于集中的几个节点之上。\n最后，再平衡是一个很昂贵的操作，我们通常希望数据系统给出一个方案，由人工确认后再执行，可以有效防止各种意外。例如：你也不想正在用户访问高点时进行分区吧。\n2.2.4 分区的请求路由 前面介绍了很多分区相关的知识，但是从客户端的角度来看，分区后该怎么找到数据的位置呢，更何况还有可能发生再平衡，数据分区的关系还会发生变化。这其实属于一类典型的服务发现问题，主要有如下方案：\n允许客户端链接任意节点，由分区节点转发请求； 请求路由层，路由层感知分区状态； 客户端感知分区状态。 无论是分区节点处理、路由层处理还是客户端处理，都需要其感知分区状态，也就是维护一份映射表，这要求其感知分区与节点对应关系以及变化；更糟糕的是，节点、路由层实例和客户端往往都不止一个，要让他们有统一的分区状态视图。\n显然，涉及到多节点统一，就离不开共识；除了内置共识算法处理外，还可以通过 ZooKeeper 等独立协调服务（外挂的共识服务）来进行管理。\n由于共识部分内容较为独立且庞大，可以参考我写的：分布式系统设计思路总览。\n2.2.5 并行查询优化 由于分区后相当于有多个独立运行的小数据库，为了充分运用其性能，可以将复杂的查询分解成许多执行阶段和分区，在不同的节点上执行。\n结语 至此，关于数据系统的通用性与可用性的简单探讨便结束了。考虑到事务虽然基础，但其实是数据库为了减轻应用层心智负担而承担的功能，而且内容较多，因此单独在后续文章中详细介绍：数据系统事务的简单探讨；同样的，共识也是很大的一个独立领域，因此在分布式系统设计思路总览这篇文章中作为重点进行详细介绍。\n","permalink":"https://xinrea.cn/posts/brief-notes-about-data-system/","summary":"\u003ch2 id=\"前言\"\u003e前言\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003e大多数应用程序是通过一层一层叠加数据模型来构建的，如果某一层使用到了大量的数据，那么就需要一个数据系统来进行管理。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e怎么理解上面这句话呢，在一整个应用系统中，一条数据可能由不同的数据模型来表示，例如：\u003c/p\u003e","title":"有关数据系统的一些简要笔记"},{"content":"在最近刷到了挺多推销 Cursor IDE 的视频和文章，大多都将其效果描述得十分夸张。然而自己是 GitHub Copilot 内测时期就开始使用的用户了，因此很想将两者进行对比。在此记录一下对于 Cursor IDE 的简单体验。\n首先 Cursor IDE 本身是基于开源的编辑器 vscode 的，因此相关的配置和插件都可以一键导入并自动加载，但是要注意的是 Cursor IDE 本身添加了许多自定义快捷键用于快捷的 AI 代码编辑，因此相关的一些插件可能会有冲突，推荐不导入 vscode 的插件配置，从干净的配置重新开始；况且在使用 Cursor IDE 时有很多原有的操作都不需要了，很多插件也就失去了作用。\n我尝试使用 Cursor IDE 进行了一个简单 HTML/JS 2d 游戏的从零构建，这与大多数相关推广的视频和文章类似：“用 AI 从 0 构建了 xxxx”。当然，很可能这套模式对于我这个项目不太擅长，因此我的体验范围十分局限，仅供参考。\n首先是项目的从零搭建，Cursor IDE 并不能帮你构建项目目录结构，在 AI 给出初始的建议后，你可以按照它的提示手动创建目录和文件。关于初始的一些代码内容，当你首次想要将修改应用到文件时，并不会自动选择对应的文件进行修改，而是选择你当前打开的文件；因此在项目初始阶段，有一些类似于初始绑定的操作需要你手动进行，当然后续就不再需要了。考虑到项目初始阶段结构并不复杂，这一点不难接受。\n后续你可以通过与 AI 对话的方式，逐步修改项目的代码，逐渐完善项目。AI 会根据你提出的需求，整理出代码修改建议以及这样修改的原因，通过简单点击的 Apply 按钮，AI 便可以通过修改建议以及现在的代码生成一份 patch，你可以 review 各处的代码修改，选择 Accept 或是 Deny。基本上自己的工作变成了 review 代码。与 Copilot 不同，这里的修改建议是项目全局的，也就是可以跨文件的，在应用修改让你 review 时也会自动切换到相应的文件；相比下来，Copilot 确实只停留在了“自动补全”上。\n操作基本都符合直觉，但是要注意的是，如果你没有完全按照 AI 的建议进行修改调整，比如在应用 AI 的修改建议后又在此基础上进行了调整，那么在同一个对话上下文下，它并不能感知这个修改；当你后续提出其他需求后，生成的修改建议中会默默将你的修改替换成 AI 建议的形式。因此在体验过程中，如果你在途中有了别的修改想法，那么要么告诉 AI 让其生成修改，要么重新发起对话，让 AI 忘记之前固执的修改建议。从这一点问题上来看，也能够理解出现这个问题的原因：Cursor IDE 显然不可能在上下文中引入所有的项目文件，因此需要在上下文中手动让 AI 感知你的变更，这也是体验过程中比较难受的一点。\n随着代码量的提升，如果修改建议涉及的文件较大且分布这个文件的不同位置，Patch 生成需要等待较长的时间，此时会感到整个流程被阻塞，很不顺畅；当然，也可以让 AI 将代码进行拆分，避免出现单个文件过大的问题。经拆分后，后续处理都会变快很多。\n最后，AI 生成的代码当然不是完美的，会有各种各样的问题，我尝试在体验过程中尽量不阅读代码，而是全程通过反馈问题让 AI 自行处理。在处理的过程中，AI 会生成一些日志代码，让你尝试运行后查看日志反馈给它以定位问题的位置，这种体验还是挺自然的。\n总的来说，Cursor IDE 并没有推广视频和文章里面展现的那么夸张，但是相较于 Copilot 来说确实有提升。与其说切换到 Cursor IDE 进行项目开发，不如说更期望 Copilot 能够借鉴 Cursor IDE 的功能进行改进。目前来说 Cursor IDE 的主要功能体现在不用手动 Copy/Paste，而是通过 AI 生成 Patch 让用户来 Review，配合项目范围内的上下文感知，展现出了不错的效果；但是包装成“AI IDE”这一概念，感觉还是有点夸大其词了。\n","permalink":"https://xinrea.cn/posts/cursor-ide-experience/","summary":"\u003cp\u003e在最近刷到了挺多推销 Cursor IDE 的视频和文章，大多都将其效果描述得十分夸张。然而自己是 GitHub Copilot 内测时期就开始使用的用户了，因此很想将两者进行对比。在此记录一下对于 Cursor IDE 的简单体验。\u003c/p\u003e","title":"Cursor IDE 的简单体验"},{"content":"近日在处理 C++ Win32 程序异常时，采用 Minidump 来保存程序崩溃时的栈记录，生成的 dmp 文件保存在配置数据目录下。 如果程序启动时检查发现存在 dmp 文件，则弹出提示框让用户选择路径来保存该文件，主要逻辑如下所示：\n1LONG WINAPI unhandled_handler(EXCEPTION_POINTERS* e) { 2 const wstring dumpfile = LAppDefine::documentPath + L\u0026#34;/minidump.dmp\u0026#34;; 3 HANDLE hFile = CreateFile(dumpfile.c_str(), GENERIC_WRITE, 0, NULL, CREATE_ALWAYS, FILE_ATTRIBUTE_NORMAL, NULL); 4 if (hFile \u0026amp;\u0026amp; (hFile != INVALID_HANDLE_VALUE)) { 5 MINIDUMP_EXCEPTION_INFORMATION mdei; 6 mdei.ThreadId = GetCurrentThreadId(); 7 mdei.ExceptionPointers = e; 8 mdei.ClientPointers = FALSE; 9 10 MiniDumpWriteDump(GetCurrentProcess(), GetCurrentProcessId(), 11 hFile, MiniDumpNormal, \u0026amp;mdei, NULL, NULL); 12 CloseHandle(hFile); 13 } 14 return EXCEPTION_CONTINUE_SEARCH; 15} 16 17int main() { 18 SetUnhandledExceptionFilter(unhandled_handler); 19 // ... 20 if (std::filesystem::exists(std::filesystem::path(filepath))) { 21 OPENFILENAME ofn; // Common dialog box structure 22 wchar_t szFile[260] = L\u0026#34;minidump.dmp\\0\u0026#34;; // Buffer for file name 23 24 ZeroMemory(\u0026amp;ofn, sizeof(ofn)); 25 ofn.lStructSize = sizeof(ofn); 26 ofn.hwndOwner = nullptr; 27 ofn.lpstrFile = szFile; 28 ofn.nMaxFile = sizeof(szFile) / sizeof(wchar_t); 29 ofn.lpstrFilter = L\u0026#34;Dump Files\\0*.dmp\\0\u0026#34;; 30 ofn.nFilterIndex = 1; 31 ofn.lpstrFileTitle = NULL; 32 ofn.nMaxFileTitle = 0; 33 ofn.lpstrInitialDir = NULL; 34 ofn.Flags = OFN_PATHMUSTEXIST | OFN_OVERWRITEPROMPT; 35 36 if (GetSaveFileName(\u0026amp;ofn) == TRUE) { 37 CopyFile(filepath.c_str(), ofn.lpstrFile, TRUE); 38 DeleteFile(filepath.c_str()); 39 } 40 } 41 // ... 42 struct stat statBuf {}; 43 if (stat(relative_path, \u0026amp;statBuf) == 0) { 44 //... 45 } 46} 实现后程序出现了奇怪的问题，当有崩溃报告存在时，后续在通过 stat 获取其他文件时必然失败，因此还导致了其他地方发生崩溃，继而程序永远无法正常启动。\n后来经过逐步调试，发现只要 GetSaveFileName() 调用成功（用户选定路径并确认）就会导致后续的 stat 出现问题。考虑到 stat 使用的路径为相对路径，猜测 GetSaveFileName() 会改变程序当前的工作目录，且经查询文档确认了这一点。\n而 ofn.Flags 也提供了解决这一问题的方法：OFN_NOCHANGEDIR。\n","permalink":"https://xinrea.cn/posts/getsavefilename/","summary":"\u003cp\u003e近日在处理 C++ Win32 程序异常时，采用 \u003ca href=\"https://learn.microsoft.com/en-us/windows/win32/debug/minidump-files\"\u003eMinidump\u003c/a\u003e 来保存程序崩溃时的栈记录，生成的 dmp 文件保存在配置数据目录下。\n如果程序启动时检查发现存在 dmp 文件，则弹出提示框让用户选择路径来保存该文件，主要逻辑如下所示：\u003c/p\u003e","title":"GetSaveFileName 引发的工作目录变更"},{"content":"在工作中经常需要为虚拟机新增磁盘设备，为了在不 reboot 的情况下使用这些新磁盘，需要对新磁盘进行检测。而检测的方法是如下命令（方法来自 Stack Exchange）：\n1for host in /sys/class/scsi_host/*; do echo \u0026#34;- - -\u0026#34; | sudo tee $host/scan; ls /dev/sd* ; done 显然起作用的部分是\n1echo \u0026#34;- - -\u0026#34; \u0026gt; /sys/class/scsi_host/host*/scan 首先我们得需要了解硬盘的相关接口以及一些术语：\nSCSI（Small Computer System Interface）：包含了物理、电气、协议和命令等多个方面规范的连接和通讯的接口标准；SCSI 硬盘不集成控制器，控制器要么集成在主机要么通过主机的扩展槽来接入； IDE（Intergrated Drive Electronics）：字面上可以看得出来，表示的是控制器和盘体集成一体式的硬盘接口技术；但是后来在提到磁盘时，等价为了 ATA（PATA）； ATA（Advanced Technology Attachment）：IDE 的概念还是比较宽泛，在广泛应用后，其所使用的一些规范被归纳，应用在了 IBM AT 计算机上，被称之为 ATA 接口标准；而 IBM AT 的成功使得 ATA 成为了 IDE 技术的代表，在标准成熟后形成了 ATA/ATAPI（ATA Packet Interface）标准； SATA（Serial ATA）：随着技术的发展，传输速率越来越快，传统并口 ATA（PATA，Parallel ATA）遇到了信号串扰和信号对齐等问题，限制了速率的进一步提高；因此在 2003 年 ATA 出现了串口的版本 SATA 1.0，并且在 2004 年立马推出了 SATA 2.0，提升了传输速率以及支持了热插拔，显然，在此之前 PATA 以及 SATA 1.0 是不支持热插拔的；2009 年推出了 SATA 3.0，每一个版本的传输速率都几乎翻倍； AHCI（Advanced Host Controller Interface）：引入于 2004 年，是一种驱动层面的数据传输协议规范；用于取代原有的 IDE（PATA）所使用的软件接口标准，以充分发挥 SATA 物理接口的实力。 所以从 IDE 宽泛的本义来看，SATA/PATA 这些都应该属于 IDE。\n到此为止，这些接口和控制标准都是为 HDD 所制定的；但近些年来 SSD 高速发展，在初期只是通过 SATA 接口来使用 SSD，后来便很快碰到了瓶颈，需要专门为 SSD 制定新的接口及控制标准，以充分发挥 SSD 的性能优势。\nM.2：是一种物理标准，有 Socket 2 和 Socket 3 两种；Socket 2 可以走 SATA 3.0 或者 PCI-E 3.0x2（物理接口并不完全决定数据经过哪些通道到达内存或者 CPU，所以 M.2 这种物理接口是可以使用 SATA 3.0 的通道的；但是显然 SATA 3.0 接口是走的 SATA 3.0 的通道）；Socket 3 可以走 PCI-E 3.0x4； NVMe（None-Volatile Memory Express）：是一种通信接口和驱动程序，对标 AHCI，是为 SSD 数据走 PCI-E 通道设计和优化的； 说到这，可以看出 M.2 SSD 如果是 Socket 2 且走 SATA 3.0 通道，用的是 AHCI 协议；如果走 PCI-E 通道，就会用到 NVMe 协议。\n从路径 /sys/class/scsi_host/host*/scan 可以看出，这应当是触发了 SCSI 适配器的扫描功能，但是结合上面的知识，会产生一个新的问题：为什么在对 scsi_host 下的控制器进行扫描，就能够检测新的硬盘呢，现在不都是 IDE 或者 AHCI 吗？\n实际上 Linux 内核为了方便各种各种类型硬盘的接入，将它们抽象化为了统一的 SCSI 接口，便于上层应用的兼容，甚至 USB 磁盘设备也可能显示在这个目录下面，但最终还是由具体的驱动来与硬件进行交互的；显然，这也不是强制的，你可以专门针对 IDE 或者 SATA 进行开发。\n对于每个 SCSI 设备，有确定的四元组来表示它：\nSCSI adapter number [host] channel number [bus] id number [target] lun [lun] LUN（Logical Unit Number）\n因为 SCSI 控制器可能不止一个，因此需要 SCSI adapter number [host] 来指明；在前面提到的命令中，路径中的 host* 指明了这一点；每个控制器上可以有多条通道，这可以是物理上的划分，也可以是逻辑上的划分；然后每条通道上又可以连接多个 SCSI 设备内，因此需要 id 来区分；每个设备上又有多个 LUN，因此需要 lun 来进行区别，可见 SCSI 的最小控制单位是 LUN。\n扫描指令参数 \u0026ldquo;- - -\u0026rdquo; 其实就是类似于 \u0026ldquo;* * *\u0026quot;，表示在每个 channel，每个 target 上检测每个 lun，以发现新连接的磁盘设备。\nSATA 设备由于没有 LUN 的概念，因此在 SCSI 抽象中 LUN 只会有一个。\n","permalink":"https://xinrea.cn/posts/from-disk-detect-to-development-of-interface/","summary":"\u003cp\u003e在工作中经常需要为虚拟机新增磁盘设备，为了在不 reboot 的情况下使用这些新磁盘，需要对新磁盘进行检测。而检测的方法是如下命令（方法来自 \u003ca href=\"https://unix.stackexchange.com/questions/404405/how-to-detect-new-hard-disk-attached-without-rebooting\"\u003eStack Exchange\u003c/a\u003e）：\u003c/p\u003e","title":"从新硬盘检测到硬盘接口的发展"},{"content":"最近在用 Go 刷算法题的过程中，切实体验到了三目运算缺失的痛苦。正好之前在探讨 Go 中 init 的处理时，了解了一些 Go 编译器的具体工作流程，因此有能力在其基础上进行修改了，于是便想着为 Go 添加三目运算。\n实验仓库为 go-with-ternary，Commit 为 compiler: add ternary support\n修改所基于的分支为 release-branch.go1.21，版本为 1.21rc2；但是在写这篇文章的过程中，Go 1.21 正式版本于 2023/08/08 发布；而且修改未涉及 go/types，导致各种 lint 工具暂时不支持三目运算（但是能够正常编译运行）。基于以上原因，将会在 release 版本代码的基础上进行修改，并加上 lint 工具的支持。\n","permalink":"https://xinrea.cn/posts/ternary-in-go/","summary":"\u003cp\u003e最近在用 Go 刷算法题的过程中，切实体验到了三目运算缺失的痛苦。正好之前在探讨 Go 中 init 的处理时，了解了一些 Go 编译器的具体工作流程，因此有能力在其基础上进行修改了，于是便想着为 Go 添加三目运算。\u003c/p\u003e","title":"为 Go 添加三目运算符"},{"content":"CAP 定理 CAP 原则又称 CAP 定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）。CAP 原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾。\n显然分区容错性 P 是必须要满足的，因为在分布式系统中网络是不可靠的，这意味着系统很可能会被分割成多个区域（也等效于节点失效，因为客观上无法分辨是网络原因还是节点原因）。如果此时不能保证分区容错性，那么也就是所一旦发生分区（或者说节点失效），分布式系统就无法正常工作了；由于分布式系统中含有许多节点，这会导致系统的故障率远远大于单体，这显然是不能接受的，与使用分布式系统的初衷背道而驰。\n因此在满足 P 的前提下，也就是说当节点失效时，C 和 A 之间的取舍就成为了分布式系统设计中的核心问题。在实际的分布式系统中，我们看到的所有设计都是围绕这两点来进行的。\n从单体到多体 假设我们有一个服务 A 运行在单个节点上，它的功能是 resp = f(req)。这个服务十分重要，我们希望它能够 7x24 小时运行，不间断地提供服务。那么我们就需要考虑如何保证这个服务的可用性。\n如果服务 A 是无状态的，也就是说服务 A 的响应 resp 只取决于 req，而与之前的请求无关，那么我们可以通过复制的方式来保证服务的可用性。我们可以在任意多的节点上运行服务 A，然后将请求以任意形式分发到可用的节点上，从而保证服务的可用性。这类无状态服务处理起来比较简单，因此在微服务拆分时，我们尽量将服务拆分成无状态的服务，这样可以大大降低系统的复杂度。\n但是，在实际中比较重要的服务往往是有状态的，也就是说服务 A 的响应 resp 不仅取决于 req，还取决于之前的请求。\n1. 备份（Master-Slave/Leader-Follower/Primary-Replica） 针对有状态服务，最简单的方法就是备份，我们可以在另一个节点上运行一个服务 B，它的功能也是 resp = f(req)。当请求到达时，我们将请求同时发送到 A 和 B，然后将 A 或者 B 的响应返回给客户端。这样，B 节点的状态与 A 节点一致，当 A 节点发生故障时，我们可以切换到 B 节点，让其接替服务。我们称 A、B 这些节点组成了一个集群。\n想想这样一个场景，请求到达集群后，由于网络原因（或者故障，集群视角无法区分），其中一个节点没有响应这个请求，那么这意味着这个节点的状态与其他节点不一致，这时我们应该怎么办呢？\n1.1 拥抱一致性 保证强一致性（线性一致性）可以想到以下几种处理方法\n对外响应处理失败，那么对于集群内的 N 个节点，一旦有一个节点出现错误，其他所有节点都要撤销操作，保证与失败的节点状态一致 不断重试，最终对外响应处理成功。但是这样会导致请求的响应时间变长，很有可能请求永远也不会成功 对外响应处理成功，剔除故障节点，将故障节点定义到一致性范围之外。这样做的问题是，如果故障节点恢复了，那么它的状态就会与其他节点不一致，一致性要求集群内节点必须与集群内其他节点状态一致，这意味着故障节点永远无法恢复；如果要加入或者恢复节点，都要停止对外服务，等待集群内所有节点状态一致再对外开放。遗憾的是，节点故障在分布式系统中是常态，这样做显然是不可行的 可见为了保证一致性，我们必须要牺牲可用性，这与 CAP 原则是一致的。\n1.2 拥抱可用性 如果我们选择可用性，那么我们可以采取以下处理方法\n如果部分节点处理成功，那么就对外响应处理成功 这里的 部分 可以是一个，可以是大部分，可以是权重最高的几个，也可以是随机的几个，可以自由选择合适的策略。\n这样做的问题是，后续请求发送到状态不一致的节点上时，处理很可能是错误的；好处是可用性大大提升，至少请求都及时处理了。\n虽然能够完成请求，但是如果对请求的处理都是错的，那么可用性再高也没有意义；还好这里 错 的定义还有可商量的余地，使得我们可以在一致性和可用性之间做出权衡。\n1.3 拥抱最终一致性 其实在强一致性（线性一致性）到最终一致性之间，还有许多不同等级的一致性；在此直接考虑要求最低的最终一致性，是因为在实际中最多考虑的还是可用性，因此在一致性方面牺牲了许多。\n最终一致性是指，在没有新的请求时，集群内的节点最终会达到一致状态，显然这里的时间差越短越好。实际上，可以结合 1.1 和 1.2 的方法来实现这一点。\n回顾以下两种方法：\n[一致性] 对外响应处理成功，剔除故障节点，将故障节点定义到一致性范围之外 [可用性] 如果部分节点处理成功，那么就对外响应处理成功 一致性方面，我们定义符合最终一致性的节点集合为 E，其中的强一致性节点组成集合 C，那么有\n$$C \\subseteq E$$集群的状态为其中状态最新的节点状态。根据请求的类别，我们可以将请求分为两类：\n强一致性请求 CR：例如涉及到写操作的请求，这类请求只能发送到强一致性节点集合 C 上，原因是：写操作需要改变集群的状态，将集群看作状态机，初始状态 Sk 在收到操作 Wj 后，状态改变为 Sk+1；同理要让状态最终一致，那么要求 Wj 必须发送到状态为 Sk 的节点上，这样才能保证集群最终状态一致；即涉及到状态改变的操作，只能发送到 C 上 最终一致性请求 ER：首先这类请求不涉及集群状态变化，可以发送到最终一致性节点集合 E 上；其次要求可以容忍读取到旧数据 我们来看看强一致性节点集合 C 的性质：\n从 1.1 中得知，对于强一致性节点集合 C，C 中元素越多，集群处理强一致性请求的可用性越低\n其中红线假设单节点可用性 99%；蓝线假设单节点可用性 99.9%；绿线假设单节点可用性 99.99%；紫线为可用性 90% 线。\n可见扩展 C 将使得集群可用性急剧下跌，且十分依靠单节点可用性。另一方面，在实现时，需要将 CR 发送到 C 中的每一个节点，造成了大量的带宽浪费；其次，为了维护 C，需要实现一个中间件用于接收分发 CR，并在节点故障时将其移出 C，而且需要统计所有 C 中节点的响应结果，集群对于 CR 的吞吐性能为 C 中各节点性能的最小值，造成了吞吐性能的急剧下滑；最后，中间件如果出现问题会导致集群不可用，因此中间件还需要集群化。\n由于以上的众多原因，实际上在集群中 C 往往只包含一个节点 C0，用于集群处理 CR 请求，这样做有以下优缺点：\n优点：\nE 节点同步简单，集群状态等同于 C0 的状态；其他节点与 C0 同步即可 CR 请求处理简单，直接交给 C0 处理即可 缺点：\nC0 挂掉会导致集群无法对 CR 请求提供服务 分布式中没有十全十美的解决方案，都需要有所取舍，但最终有很多项目都选用了 C 只包含一个节点的形式。例如 MySQL、Redis、Kafka、Zookeeper 以及使用 Raft 算法的其他项目等等，都使用了这一设计思想。\n当然要使用这种设计，得解决一个核心问题，也就是弥补该设计的缺点：C0挂掉会导致集群无法对 CR 请求提供服务。显然我们不可能启用全新的节点来替代，而集群中有着许多接近 C0 的状态的最终一致性节点，因此可以从它们中选出新的 C0。当然，如果选出的节点状态与原来的 C0 不同，则会导致数据的丢失，好在有着一系列机制来保证这些节点的最终一致性，而且 C0 不可用时集群状态不会再更新，相当于提供了一个等待同步的机会，因此可以尽可能避免数据丢失情况的产生，这一点后续再详细阐述。\n因此现在我们遇到了一个新问题，当 C0 挂掉时，该如何切换节点来替代 C0。\n1.4 如何切换 1.4.1 人工干预 切换的方法有很多种，其中最简单的方法就是人工干预，当 A 节点发生故障时，我们手动将请求切换到 B 节点。这种方法的缺点是服务无法使用，直到人工干预完成。这种方法看起来很蠢，但是如果不是使用在故障处理上，还是有可取之处的。\n例如 MySQL 集群的迁移就可以用到这种方式；首先将新的节点加入最终一致性节点集合 E，也就是作为 Slave 节点来同步 Master 节点的数据。待到数据同步完成，ER 操作实际上已经可以切换到新节点上了；最后手动在新节点中启用一个 Master 节点，替换旧 Master 节点，然后将 CR 请求切换到新 Master 节点即可。\n显然最后切换 Master 节点时与 C0 节点故障的处理等效，因此集群无法提供服务。\n1.4.2 哨兵机制 为了自动化完成前面提到的人工干预方法，引入了哨兵机制。其本质上跟人工干预相同，不过是使用一个服务来替代人完成这一操作。优点是无需在原有服务上做修改，哨兵服务与原服务独立运行。\n显然，哨兵服务的可用性也需要用集群来保证，因此 Redis 哨兵模式架构如下所示：\ngraph TD subgraph Sentinel A1(Sentinel Leader) \u0026lt;--\u0026gt; A2(Sentinel 1) A3(Sentinel 2) \u0026lt;--\u0026gt; A2 A3(Sentinel 2) \u0026lt;--\u0026gt; A1 end subgraph Redis B(Master) \u0026lt;--\u0026gt; C(Slave 1) B \u0026lt;--\u0026gt; D(Slave 2) end Sentinel --\u0026gt; Redis 哨兵服务监控所有 Redis 节点的状态，以便当 Master 节点宕机后进行自动切换。这样就解决了 Redis Master 节点宕机后新 Matser 的切换问题。\n但是和 Redis 集群一样，哨兵服务在执行各种操作时，执行操作的主体只能是一个节点（Leader），其他节点作为备份（Follower），以达成一定的一致性和可用性。那么，当 Sentinel Leader 挂掉后，该如何切换新的 Leader 节点呢？如果还是使用哨兵机制的话，那么需要启用 Sentinel 的 Sentinel 服务，最终会无限套娃下去。\n为了打破这一套娃循环，我们需要集群本身具有 C0 节点的切换机制，不能依赖外部服务。\n1.4.3 分布式共识算法 前面在描述哨兵集群中的节点时，使用的是 Leader/Follower 而不是 Master/Slave，这是因为哨兵集群使用 Raft 算法实现了 C0 的自动切换。通常我们将使用选举算法实现的 C0 称为 Leader，实际上集群视角来看都为主从模式，只是“主”的故障迁移使用了不同的机制来实现；避免使用 Master 和 Slave 等词汇也有政治正确的体现（例如 GitHub 将主分支默认名称 Master 更改为 Main)。\n首先要说明，此处不讨论拜占庭将军问题，因为我们认为对集群中节点具有完全的控制权，因此集群内部的节点均按照规定的机制运行，出现的所有错误情况均是可预测的。\n因此这里讨论的分布式共识没有 BTC 等区块链项目的共识机制严格，但仍需考虑各种异常情况的处理；在这一点也能窥见区块链的魅力，在完全混沌和不可信的环境中构建一份共识。\nPaxos 算法 提到分布式共识算法就得从 Paxos 算法 说起。Paxos 算法模拟了一个小岛上通过决议的流程，一个值的确定需要多数人发起提案（Prepare 阶段）后，得到多数人的同意（Accept 阶段）。其中每个人都要维护一个 Proposal ID，已确保只处理自身视角里最新的 Proposal，Proposal ID 将会在 Prepare 和 Accept 阶段中更新。\n这样的 Basic Paxos 算法的问题在于，只要多数人就可发起提案，因此很可能前一个提案还未处理完便发起了新的提案，导致 Accept 阶段有些人得知了新的提案便不再处理旧提案，使得旧提案无法得到多数人的确认，此时若再次发起提案，那么刚刚的新提案也会遇到同样的问题，造成活锁；同时，这样的流程仅能确定一个值，无法满足实际需求。\n为了解决以上 Basic Paxos 算法的缺陷，很自然的提出了 Multi-Paxos 算法。为了解决只能确定一个值的问题，我们对每一个需要决定的值都使用一次 Paxos 算法；为了解决活锁的问题，我们限制提案只能由一个人发起，这个人叫做 Leader，代价是损失了一定的可用性，一旦 Leader 宕机算法将无法正常进行。\n同时，由于 Leader 只有一个且算法将多次进行，那么在一开始提前使用 Paxos 算法确定好 Leader 后，后续的 Prepare 阶段都可以省略掉了；当 Leader 宕机后使用 Paxos 算法再选一个出来就可以了。这样还解决了算法 Prepare 阶段网络 IO 的耗时，提高了算法的效率。\nLeader 的选举还是使用的 Basic Paxos 算法，但是由于目的明确且易于控制，不会连续产生多个 Proposal 导致活锁。\nRaft 算法 Paxos 算法中简单提到了可以使用 Paxos 自身来实现 Leader 的选举，也简单提到了可以使用多个 Paxos 实例来完成多个值（例如 Log）的复制，但并未详细设计和证明这些使用场景下的细节问题。虽然有 Paxos 算法的正确性证明，但是在实际使用中，很难严格基于 Paxos 实现一个可用的系统，基本上都是有所变动，这导致 Paxos 算法的正确性与实际系统的正确性脱钩。为了解决这一问题，Raft 算法诞生了。\nRaft 是根据各种分布式系统的实际需求来构建的，因此设计时注重模块化和实用性，而且要易于理解，这样在实现、维护甚至改动时，才能确保系统的正确性。Raft 主要从 Leader 选举、日志复制和安全性保证三个方面进行了设计，还思考了实际系统所需的成员变更和日志压缩问题的处理方案。\nThe Raft Consensus Algorithm\nRaft Visualizations\n我们在此对 Raft 算法进行简单探讨：\nLeader Election（RequestVote RPC）：成员仅有三种可能的状态，Leader、Candidate 和 Follower；成员初始状态为 Follower，一旦来自 Leader 的心跳超时，便会发起选举，变为 Candidate。每次发起选举时，便会进入一个新的单调递增的 Term，这个 Term 起到了逻辑时钟的作用；这也意味着，无论成员处于什么状态，一旦接收到更高 Term 的消息，便能意识到自己已经过时，将自己转换成 Follower。Raft 通过引入随机延迟要解决 Paxos 中活锁的问题。 Log Replication（AppendEntries RPC）：针对 Paxos 单值共识的问题，Raft 采用日志复制与状态机来实现多个值的共识；显然，只要保证了 Log 共识，状态机的状态自然能保证一致。Raft 限制 Log 的流动方向为 Leader -\u0026gt; Follower，Leader 了解到 Log 已经被大多数节点存储后，便会将其提交，Log 一旦提交后永不撤销，这意味着系统内已 Commit 的 Log Index 上的内容完全一致；同时该 Index 之前的 Log 页保证一致。 Safty Argument：安全性的讨论逻辑其实很简单，我们一步步来。首先如果不发生 Leader 切换，显然一致性是得到保证的；如果发生切换，要保证一致性的话，等同于新选举出的 Leader\u0026rsquo; 需要与 Leader 保持一致性；a.从日志复制流程中得知，最新提交的日志必定存在于大多数节点上，b.从选举流程中得知，选举成功要获得大多数节点的投票，那么里面至少有一个节点 S 包含了最新提交的日志，c.投票只能投给日志比自己新的节点，因此选举出的 Leader\u0026rsquo;，日志至少比节点 S 新，d.新选举出的 Leader\u0026rsquo;，必然包含了最新提交的日志，与 Leader 是保持一致的。既然 Leader 节点薪火相传，保持一致，那么其他没有选举成功的节点有可能日志存在异常，因此允许 Leader 覆盖修复 Follower 的未提交日志以进行修复。 日志压缩（InstallSnapshot RPC）：显然，Append Log 很适合创建 Snapshot 来实现压缩，为此 Raft 还设计了 InstallSnapshot RPC 来让 Leader 向 Follower 推送统一的 Snapshot。 集群成员变更：在原有节点节点的基础上进行变动时，我们需要保证这个变动能传达到集群内的每一个节点，形成共识，因此配置的变更是通过特殊的 Log Entry 来实现的；由于这一过程在集群中是逐步更新的，我们还要确保更新过程中，在过渡态内不出现脑裂；为此 Raft 考虑了单节点成员变更和多节点联合共识两种方案。 要注意的是，本文中列举分布式共识算法是为了实现一致性节点的选举，分布式共识算法本身是强一致性的；在此基础上，我们选择一致性节点后，对外提供服务时可以在可用性与一致性之间做取舍：非一致性节点在拿到请求时，能够通过共识模块知道哪个节点是一致性节点，可以根据设置需求，设定转发所有操作或是只转发写操作。\n","permalink":"https://xinrea.cn/posts/distribute-system-design/","summary":"\u003ch2 id=\"cap-定理\"\u003eCAP 定理\u003c/h2\u003e\n\u003cp\u003eCAP 原则又称 CAP 定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）。CAP 原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾。\u003c/p\u003e","title":"分布式系统设计思路总览"},{"content":" 本文中引用的源码均标注了 Golang 源码仓库链接，branch 为 release-branch.go1.21（本文在编写时 Go 1.21 还未正式发布，正式版可能会有少量变化）。\ninit() 在不规范使用情况下产生的现象 在同一个 go 文件里，初始相关操作的执行顺序是 const -\u0026gt; var -\u0026gt; init()。显然，如果同一个文件里有多个 init()，那么将按照声明顺序来执行。\n如果 Package 的 init() 分布在不同的文件里，将会按照什么顺序来执行呢？\n有如下场景：\n1main.go 2a/b.go 3a/c.go 1// main.go 2package main 3 4import \u0026#34;go-init/a\u0026#34; 5 6func main() { 7 a.A() 8} 1// a/b.go 2package a 3 4func init() { 5 println(\u0026#34;b init\u0026#34;) 6} 7 8func A() int { 9 println(\u0026#34;A\u0026#34;) 10 return 0 11} 1// a/c.go 2package a 3 4func init() { 5 println(\u0026#34;c init\u0026#34;) 6} 执行 go run main.go，得到输出：\n1b init 2c init 3A 接下来将 a/b.go 改名为 a/d.go，再次执行 go run main.go，输出：\n1c init 2b init 3A 可以看到有 [现象]：a/b.go 和 a/c.go 的 init() 函数的执行顺序是按照文件名的字母顺序来的，将 a/b.go 改名后，其文件名顺序排在了 a/c.go 之后，最终 init() 执行也排在了之后。\n还有更多复杂的情况，例如：\n如果 import 的包之间存在依赖关系，那么这些包的 init() 的执行顺序是怎样的？ 如果 Package 的 init() 分布在不同的文件里，而且这些文件里有交叉依赖的 var 全局变量，那么 init() 和这些全局变量初始化的执行顺序又是怎样的？ 实际上，要真正弄清楚这些，需要深入 Go 编译器，从根源弄清原理。init() 的处理是 Go 编译过程中的重要一环。\n编译的起点 gc.Main() Golang 编译器相关源码位于 go/src/cmd/compile/。\nGo 编译处理的单位是 Package，得到的结果是 Object 文件。在一次编译过程开始时会读取 Package 中所有文件内容进行词法和语法分析。我们很容易就能找到编译器的入口文件 main.go：\n1// https://github.com/golang/go/blob/d8117459c513e048eb72f11988d5416110dff359/src/cmd/compile/main.go#L45 2func main() { 3 // disable timestamps for reproducible output 4 log.SetFlags(0) 5 log.SetPrefix(\u0026#34;compile: \u0026#34;) 6 7 buildcfg.Check() 8 archInit, ok := archInits[buildcfg.GOARCH] 9 if !ok { 10 fmt.Fprintf(os.Stderr, \u0026#34;compile: unknown architecture %q\\n\u0026#34;, buildcfg.GOARCH) 11 os.Exit(2) 12 } 13 14 gc.Main(archInit) 15 base.Exit(0) 16} gc.Main() 完成了整个编译流程，其内容是本文的重点；编译流程本身比较清晰，但内容很多，在本文中主要关心 init() 相关的处理。\n为了方便理解，请先阅读编译器部分的 README.md，了解编译器的基本流程和相关概念；下面也简单介绍一下编译器的流程，补充一些细节，便于理解为什么 Go 编译器现在是这样一个结构。\n编译流程 1. Parsing 词法和语法分析得到 AST（Abstract Syntax Tree，抽象语法树），每个 AST 等价于一个源文件。关于树的结构和节点的定义见 internal/syntax/nodes.go，Go 源码中的所有元素都能在这里找到对应的结构，极其基础和重要。\n例如：\n1// X[Index[0] : Index[1] : Index[2]] 2SliceExpr struct { 3 X Expr 4 Index [3]Expr 5 // Full indicates whether this is a simple or full slice expression. 6 // In a valid AST, this is equivalent to Index[2] != nil. 7 // TODO(mdempsky): This is only needed to report the \u0026#34;3-index 8 // slice of string\u0026#34; error when Index[2] is missing. 9 Full bool 10 expr 11} 我们可以从中知道 slice[:] 操作实际上可以有三个参数，分别表示指针（新起始位置）、长度和容量。\n同时也可以看到 Comments 相关结构仍在开发之中，后续可能会加入 AST 用于生成更加结构化的文档。\n2. Type checking 类型检查，types2 是从 go/types 移植而来的，在这里需要结合发展历史来理解。\n在一开始，Go 的编译器是使用 C 来实现的。到 Go 1.5 版本，实现了自举，其中编译过程中类型检查使用的是 Go 实现的传统检查算法，位于 internal/gc/typecheck.go；同时 Go 1.5 版本在标准库里面加入了 go/types，便于开发者开发 Go 代码分析工具。随着各种代码检查工具的涌现，go/types 发展迅猛，相比较而言，internal/gc/typecheck.go 涉及编译器过于底层，发展较慢。\n直到 Go 1.17 开始开发，需要将泛型作为实验特性加入，此时编译器的类型检查已经无法满足要求，好在 go/types 已经十分成熟，借助其强大的类型推导能力，在编译器中实现了对泛型的处理；这也是 internal/types2 和 go/types 一开始相同的原因。后续对 go/types 问题的修复也应同步到 types2，这样才能保证编译器和代码分析工具的一致性，同时也相当于让更多人参与了编译器的改进；当然，编译器自身也有些特殊需求需要在 types2 中实现。由于现在有两种并行的实现，因此 internal/gc/typecheck.go 被抽取出来，成为了 internal/typecheck 包。\n在 Go 1.5 之前，编译器使用 C 实现，不存在 Go 实现的类型检查。\n在 Go 1.5 - 1.16，编译器使用 Go 实现，类型检查使用 gc/typecheck.go，官方提供了 go/types 包。\n在 Go 1.17 时泛型还只是可选项，因此编译器提供了参数 -G 来选择是否开启泛型，实际上，当 G=0 时编译器会使用旧的 typecheck 来进行类型检查；当 G=3 时使用 types2 （go/types移植而来）进行类型检查以支持泛型。\n在 Go 1.18 正式推出泛型以后，-G 参数仍然存在，只不过默认值改成了 G=3，也就是说，现在编译器默认使用 types2 进行类型检查。\n在 Go 1.19 推出后，-G 参数被移除，编译器只能使用 types2 进行类型检查。\n例如 Commit: 8fd2875：\n修改 src/go/types 后，也同步修改了 src/cmd/compile/internal/types2 下的内容。\n3. IR construction(\u0026ldquo;noding\u0026rdquo;) IR（Intermediate Representation，中间表示）是一种介于 AST 和汇编代码之间的表示，是一种更加抽象的表示，能够更好地表示语义。这一步就是将 AST 转换为 IR，这个过程称作 noding。\n在 Go 1.17 之前，并没有 IR 的概念，或者说有，但是还不叫 IR。\n在 Go 1.17，当 G=0 时，编译器可选择使用 internal/typecheck 进行类型检查，因此对应使用 noder 进行 noding；当 G=3 时，编译器使用 types2 进行类型检查，因此使用相应的新实现来进行 noding， 称之为 noder2。\n在 Go 1.18，同样可以通过 -G 参数来选择使用 internal/typecheck 或者 types2 进行类型检查，因此 noder 和 noder2 仍然是并存的。\n在 Go 1.19 之后，编译器只能使用 types2 进行类型检查，因此 noder2 也是唯一的 noding 实现。\n其实 IR 也是一种形式的 AST，被称为 GC AST（Go Compiler AST）。那么为什么要转换呢？主要原因是自 Go 1.5 实现自举时，参考旧 C 的实现来完成了 AST 上的类型检查等等后续操作；但是新的 Go 实现的词法语法分析得到的 AST 只是分别与源文件对应，还未处理 import 以及合并，并不完整；好在这个转换并不复杂。\n旧的处理流程如下所示：\n1// 旧处理流程 Go 1.5 - 1.16 2[AST1,AST2,...] := Parse([file1,file2,...]) 3 4// 处理 import，合并 AST 5IR := Noder([AST1,AST2,...]) 6 7// 类型检查 8Typecheck(IR) 9MiddleEndOP(IR) 10SSA := SSAGen(IR) 11MACHINE_CODE := CodeGen(SSA) 在 Go 1.17 引入 types2 后，由于 types2 是作用于 AST 上的，因此新的处理流程变成了：\n1// 新处理流程 Go 1.17 2[AST1,AST2,...] := Parse([file1,file2,...]) 3 4#if G=3 5 // 处理 import，类型检查，处理泛型 6 TypeInfo := Types2([AST1,AST2,...]) 7 IR := Noder2([AST1,AST2,...],TypeInfo) 8#elseif G=0 9 // 处理 import，合并 AST 10 IR := Noder([AST1,AST2,...]) 11#endif 12 13// 之后完全一致 14Typecheck(IR) 15MiddleEndOp(IR) 16SSA := SSAGen(IR) 17MACHINE_CODE := CodeGen(SSA) noder2 的实现位于 internal/ir。会发现当 G=3 时，虽然用了 internal/types2 来进行类型检查，但是后续在 IR 上还是跑了一遍 internal/typecheck，在这里有许多原因，主要是 internal/typecheck 会对 IR 进行一些修改调整，因此还需要保留，详情可以看这里的注释：internal/noder/unified.go#L51。\n在 Go 1.18 又引入了 Unified IR（GOEXPERIMENT=unified 开启），于是乎三种流程并行存在：\n1// 新处理流程 Go 1.18 2[AST1,AST2,...] := Parse([file1,file2,...]) 3 4#if Unified 5 IR := Unified([AST1,AST2,...]) 6#else 7 #if G=3 8 // 处理 import，类型检查，处理泛型 9 TypeInfo := Types2([AST1,AST2,...]) 10 IR := Noder2([AST1,AST2,...],TypeInfo) 11 #elseif G=0 12 // 处理 import，合并 AST 13 IR := Noder([AST1,AST2,...]) 14 #endif 15 Typecheck(IR) 16#endif 17 18MiddleEndOp(IR) 19SSA := SSAGen(IR) 20MACHINE_CODE := CodeGen(SSA) 在 Go 1.19 移除了 G=0 的流程：\n1// 新处理流程 Go 1.19 2[AST1,AST2,...] := Parse([file1,file2,...]) 3 4#if Unified 5 IR := Unified([AST1,AST2,...]) 6#else 7 // 处理 import，类型检查，处理泛型 8 TypeInfo := Types2([AST1,AST2,...]) 9 IR := Noder2([AST1,AST2,...],TypeInfo) 10 Typecheck(IR) 11#endif 12 13MiddleEndOp(IR) 14SSA := SSAGen(IR) 15MACHINE_CODE := CodeGen(SSA) 在 Go 1.21 正式启用了 Unified IR，因此 unified 也就是唯一的 noding 实现了，确实实现了统一，欢迎来到 Go 1.21 ！（实际上需要处理的东西其实没有改变，只是整合在了 Unified 内，因此原来的包还依然存在）\n1// 新处理流程 Go 1.21 2[AST1,AST2,...] := Parse([file1,file2,...]) 3 4IR := Unified([AST1,AST2,...]) 5 6MiddleEndOp(IR) 7SSA := SSAGen(IR) 8MACHINE_CODE := CodeGen(SSA) 下面是各种 noder 在不同版本的存在状态：\nGo 1.17 之前：noder Go 1.17: noder, noder2 Go 1.18: noder, noder2, unified Go 1.19: noder2, unified Go 1.20: noder2, unified Go 1.21: unified 4. Middle end internal/deadcode (dead code elimination) internal/inline (function call inlining) internal/devirtualize (devirtualization of known interface method calls) internal/escape (escape analysis) 5. Walk，SSA Gen 以及机器码生成 Walk 遍历 IR，拆分复杂的语句以及将语法糖转换成基础的语句 SSA Gen 将 IR 转化为 Static Single Assignment (SSA) 形式，此时还与具体的机器无关 机器码生成会根据架构以及更多机器相关的信息，对 SSA 进行优化；同时进行栈帧分配，寄存器分配，指针存活分析等等，最终经过汇编器 cmd/internal/obj 生成机器码。 流程中 init 相关的处理 前面我们了解了 Go 编译器的流程，以及其发展变化的历史。接下来我们来看看其中 init 相关的具体处理。\n1// https://github.com/golang/go/blob/d8117459c513e048eb72f11988d5416110dff359/src/cmd/compile/internal/gc/main.go#L59 2// Main parses flags and Go source files specified in the command-line 3// arguments, type-checks the parsed Go package, compiles functions to machine 4// code, and finally writes the compiled package definition to disk. 5func Main(archInit func(*ssagen.ArchInfo)) { 6 ... 7 // Parse and typecheck input. 8 noder.LoadPackage(flag.Args()) 9 ... 10 // Create \u0026#34;init\u0026#34; function for package-scope variable initialization 11 // statements, if any. 12 // 13 // Note: This needs to happen early, before any optimizations. The 14 // Go spec defines a precise order than initialization should be 15 // carried out in, and even mundane optimizations like dead code 16 // removal can skew the results (e.g., #43444). 17 pkginit.MakeInit() 18 ... 19 // Build init task, if needed. 20 if initTask := pkginit.Task(); initTask != nil { 21 typecheck.Export(initTask) 22 } 23 ... gc.Main() 流程中主要有以上三部分对 init 进行了处理，接下来我们分别看看这三部分。\nnoder.LoadPackage() 1// https://github.com/golang/go/blob/d8117459c513e048eb72f11988d5416110dff359/src/cmd/compile/internal/noder/noder.go#L27 2func LoadPackage(filenames []string) { 3 ... 4 noders := make([]*noder, len(filenames)) 5 ... 6 go func() { 7 for i, filename := range filenames { 8 ... 9 go func() { 10 ... 11 f, err := os.Open(filename) 12 ... 13 p.file, _ = syntax.Parse(fbase, f, p.error, p.pragma, syntax.CheckBranches) // errors are tracked via p.error 14 }() 15 } 16 }() 17 ... 18 unified(m, noders) 19} 可以看到 LoadPackage() 会并行的对每个文件进行读取以及词法语法分析，构建 AST。并将得到的 AST 列表传递给 unified() 进行统一处理。\n1// https://github.com/golang/go/blob/d8117459c513e048eb72f11988d5416110dff359/src/cmd/compile/internal/noder/unified.go#L71 2func unified(m posMap, noders []*noder) { 3 ... 4 data := writePkgStub(m, noders) 5 ... 6 target := typecheck.Target 7 r := localPkgReader.newReader(pkgbits.RelocMeta, pkgbits.PrivateRootIdx, pkgbits.SyncPrivate) 8 r.pkgInit(types.LocalPkg, target) 9 10 // 后面均为 `internal/typecheck` 的处理，与 init 无关 11 // Type-check any top-level assignments. We ignore non-assignments 12 // here because other declarations are typechecked as they\u0026#39;re 13 // constructed. 14 for i, ndecls := 0, len(target.Decls); i \u0026lt; ndecls; i++ { 15 switch n := target.Decls[i]; n.Op() { 16 case ir.OAS, ir.OAS2: 17 target.Decls[i] = typecheck.Stmt(n) 18 } 19 } 20 21 readBodies(target, false) 22 23 // Check that nothing snuck past typechecking. 24 for _, n := range target.Decls { 25 if n.Typecheck() == 0 { 26 base.FatalfAt(n.Pos(), \u0026#34;missed typecheck: %v\u0026#34;, n) 27 } 28 29 // For functions, check that at least their first statement (if 30 // any) was typechecked too. 31 if fn, ok := n.(*ir.Func); ok \u0026amp;\u0026amp; len(fn.Body) != 0 { 32 if stmt := fn.Body[0]; stmt.Typecheck() == 0 { 33 base.FatalfAt(stmt.Pos(), \u0026#34;missed typecheck: %v\u0026#34;, stmt) 34 } 35 } 36 } 37 ... 38} 其中 writePkgStub() 完成了类型检查。接下来的调用链有点长，在这里就不放源代码了，大致流程如下：\nwritePkgStub() -\u0026gt; noder.checkFiles -\u0026gt; conf.Check() -\u0026gt; Checker.Files() -\u0026gt; check.checkFiles()\n1// https://github.com/golang/go/blob/d8117459c513e048eb72f11988d5416110dff359/src/cmd/compile/internal/types2/check.go#L335 2func (check *Checker) checkFiles(files []*syntax.File) (err error) { 3 ... 4 print(\u0026#34;== initFiles ==\u0026#34;) 5 check.initFiles(files) 6 7 print(\u0026#34;== collectObjects ==\u0026#34;) 8 check.collectObjects() 9 10 print(\u0026#34;== packageObjects ==\u0026#34;) 11 check.packageObjects() 12 13 print(\u0026#34;== processDelayed ==\u0026#34;) 14 check.processDelayed(0) // incl. all functions 15 16 print(\u0026#34;== cleanup ==\u0026#34;) 17 check.cleanup() 18 19 print(\u0026#34;== initOrder ==\u0026#34;) 20 check.initOrder() 21 ... 22} initFiles() 用于检查文件开头的 package 语句所声明的名称是否符合要求，例如要跟当前 package 名一致，否则忽略这个文件（都经过词法语法分析了，白分析了，当然编译前就能检查出这些问题，一般不会进行到这里才发现）。\ncollectObjects() 在此处对 import 的 Package 进行了加载，并将其置于相应的 Scope 中。可以看到这里仍然是按照文件顺序在进行处理，通过 check.impMap 来缓存已经加载的 Package；同时用 pkgImports map[*Package]bool 来记录本 Package 已经引用的 Package，避免其重复加入 pkg.imports 数组。\n同时，还能从中看到一些特殊 import 的处理，例如 import . 和 import _ 以及别名。DotImport 会将 imported package 中的导出符号全部遍历导入到当前的 FileScope 中，而一般情况下是将 imported package 整个加入到当前的 FileScope 中，这样会有额外的层次结构。\n注意这里提到了 FileScope，我们知道在 Go 的同一个 Package 下，许多声明是不存在 FileScope 的，例如全局变量在一个文件中声明，另一个文件中可以直接使用；同名也会发生冲突，因为这些都在同一个 PackageScope 下。但是对于 import 操作来说，每个文件都有自己需要 import 的内容，因此需要一个 FileScope 来记录区分这些信息。\nScope 结构组织好后，还需要检查 FileScope 跟 PackageScope 之间的冲突问题，这主要是 DotImport 导致的。\n1// https://github.com/golang/go/blob/d8117459c513e048eb72f11988d5416110dff359/src/cmd/compile/internal/types2/resolver.go#L472 2// verify that objects in package and file scopes have different names 3for _, scope := range fileScopes { 4 for name, obj := range scope.elems { 5 if alt := pkg.scope.Lookup(name); alt != nil { 6 ... initOrder() 是对一些有依赖关系的全局声明进行排序，并未涉及 init 的处理，例如：\n1var ( 2 // a depends on b and c, c depends on f 3 a = b + c 4 b = 1 5 c = f() 6 7 // circular dependency 8 d = e 9 e = d 10) 在 Go 中，能够被用于初始化表达式的对象被称为 Dependency 对象，有 Const, Var, Func 这三类。先构建对象依赖关系的有向图（Directed Graph），再以每个节点的依赖数目为权重构建最小堆（MinHeap）并以此堆作为最小优先级队列（PriorityQueue），因此队列头部的对象总是依赖其它对象最少的，所以该队列的遍历顺序就是初始化的顺序，是很常规的处理思路。要注意常量的初始化比较简单，在构建时就已经确定，在这里仍然加入是为了检测循环依赖。\n1https://github.com/golang/go/blob/d8117459c513e048eb72f11988d5416110dff359/src/cmd/compile/internal/noder/unified.go#L209 2func writePkgStub(m posMap, noders []*noder) string { 3 // 类型检查 4 pkg, info := checkFiles(m, noders) 5 6 pw := newPkgWriter(m, pkg, info) 7 pw.collectDecls(noders) 8 ... 9 var sb strings.Builder 10 pw.DumpTo(\u0026amp;sb) 11 12 // At this point, we\u0026#39;re done with types2. Make sure the package is 13 // garbage collected. 14 freePackage(pkg) 15 16 return sb.String() 17} 最后再回到开始，可见 writePkgStub 包含了 internal/types2 的类型检查；类型检查会涉及到外部包的导出类型，也就是说会处理 import 语句；同时，类型检查的过程中也生成了一份 types2.Package 以及 types2.info，其中 types2.package 包含 Scope 层次信息以及每个 Scope 中的 Object 信息；types2.info 包含了类型检查中生成的类型信息；最后通过 pkgWriter 将这两个信息整合序列化为字符串，也就是最终得到的 data。\n实际上，这个 data 就是 Unified IR 的导出；接下来使用 pkgReader 将 data 重新构建为 IR，存储在 typecheck.Target。\n明明步骤紧接在一起，为什么要把 Unified IR 先 export 再 import 呢？ 这样做主要是为了将 Unified IR 与后续部分完全解耦，可以看到只要有 export data 就能够完成后续的编译工作；同时通过实现不同的 pkgReader，便可以从 export data 中提取出不同的信息。例如编译器需要从中读取完整的 IR； x/tools 下的工具需要对代码进行静态分析，那么就可以实现一个 pkgReader 来提取自己需要的信息，而不必再自己实现一遍词法语法分析以及类型检查。\n在 pkgReader 构建 IR 的过程中，遇到函数类型的 Object 时，做了如下处理：\n1// https://github.com/golang/go/blob/d8117459c513e048eb72f11988d5416110dff359/src/cmd/compile/internal/noder/reader.go#L750 2case pkgbits.ObjFunc: 3 if sym.Name == \u0026#34;init\u0026#34; { 4 sym = Renameinit() 5 } 6... 可见 init 函数是多么特殊，它会被重命名，这样就不会与其他 init 函数冲突了。Renameinit() 的实现如下：\n1// https://github.com/golang/go/blob/d8117459c513e048eb72f11988d5416110dff359/src/cmd/compile/internal/noder/noder.go#L419 2var renameinitgen int 3 4func Renameinit() *types.Sym { 5 s := typecheck.LookupNum(\u0026#34;init.\u0026#34;, renameinitgen) 6 renameinitgen++ 7 return s 8} 可见只是给了个编号，重命名成了一系列 init.0 init.1 init.2 等等的函数。\n至此 LoadPackage() 的工作就完成了。\npkginit.MakeInit() 接下来终于来到了 pkginit 包的内容。\n1// https://github.com/golang/go/blob/d8117459c513e048eb72f11988d5416110dff359/src/cmd/compile/internal/gc/main.go#L59 2// Main parses flags and Go source files specified in the command-line 3// arguments, type-checks the parsed Go package, compiles functions to machine 4// code, and finally writes the compiled package definition to disk. 5func Main(archInit func(*ssagen.ArchInfo)) { 6 ... 7 // Parse and typecheck input. 8 noder.LoadPackage(flag.Args()) 9 ... 10 // Create \u0026#34;init\u0026#34; function for package-scope variable initialization 11 // statements, if any. 12 // 13 // Note: This needs to happen early, before any optimizations. The 14 // Go spec defines a precise order than initialization should be 15 // carried out in, and even mundane optimizations like dead code 16 // removal can skew the results (e.g., #43444). 17 pkginit.MakeInit() 18 ... 19 // Build init task, if needed. 20 if initTask := pkginit.Task(); initTask != nil { 21 typecheck.Export(initTask) 22 } 23 ... 从注释也可以知道，在词法分析、语法分析以及类型检查和构造 IR 树的过程中，均未涉及代码优化。以下是 MakeInit() 的内容，关键部分使用中文进行了更详细的注释，可以对照相关方法的源码进行阅读。\n1// TODO(mdempsky): Move into noder, so that the types2-based frontends 2// can use Info.InitOrder instead. 3func MakeInit() { 4 // Init 相关的处理只涉及全局声明（Package Level），依赖关系作为有向边来构建有向图，然后进行拓扑排序。 5 nf := initOrder(typecheck.Target.Decls) 6 if len(nf) == 0 { 7 return 8 } 9 10 // Make a function that contains all the initialization statements. 11 base.Pos = nf[0].Pos() // prolog/epilog gets line number of first init stmt 12 // 查找 init 符号，如果 Package 的全局符号中没有则创建；不用担心 init 符号已经被用户的 init 函数使用，因为 IR 树在生成过程中遇到 init 会重命名为 init.0 init.1 这样的格式，前面提到 g.generate() 的时候也有说明。 13 initializers := typecheck.Lookup(\u0026#34;init\u0026#34;) 14 /* 用 init 符号声明一个新的函数，用于存放所有的初始化工作。具体实现是：此处在 IR 树中对应位置建立了新的 ONAME Node（ONAME 表示 var/func name），类型指定为 PFUNC，同时也将符号表中的 init 更新为 symFunc，表明这个符号是函数名；然后新建一个函数节点，将 ONAME Node 指向函数节点，最后将函数节点返回。 15 */ 16 fn := typecheck.DeclFunc(initializers, nil, nil, nil) 17 // 类型检查过程中生成了一个 InitTodoFunc，其作为全局初始化语句的临时上下文环境。现在将临时环境 InitTodoFunc 的内容转移到 fn。 18 for _, dcl := range typecheck.InitTodoFunc.Dcl { 19 dcl.Curfn = fn 20 } 21 fn.Dcl = append(fn.Dcl, typecheck.InitTodoFunc.Dcl...) 22 typecheck.InitTodoFunc.Dcl = nil 23 24 // Suppress useless \u0026#34;can inline\u0026#34; diagnostics. 25 // Init functions are only called dynamically. 26 fn.SetInlinabilityChecked(true) 27 28 // 配置函数体。 29 fn.Body = nf 30 typecheck.FinishFuncBody() 31 32 // 确定 fn 为函数节点 33 typecheck.Func(fn) 34 // 在 fn 的内部上下文环境下检查函数体 35 ir.WithFunc(fn, func() { 36 typecheck.Stmts(nf) 37 }) 38 // 把函数加入到 Package 的全局声明列表。 39 typecheck.Target.Decls = append(typecheck.Target.Decls, fn) 40 41 // Prepend to Inits, so it runs first, before any user-declared init 42 // functions. 43 typecheck.Target.Inits = append([]*ir.Func{fn}, typecheck.Target.Inits...) 44 45 if typecheck.InitTodoFunc.Dcl != nil { 46 // We only generate temps using InitTodoFunc if there 47 // are package-scope initialization statements, so 48 // something\u0026#39;s weird if we get here. 49 base.Fatalf(\u0026#34;InitTodoFunc still has declarations\u0026#34;) 50 } 51 typecheck.InitTodoFunc = nil 52} 旧版本中 MakeInit() 的工作是在 pkginit.Task() 中实现的，现在被抽取了出来，原因有以下几点。\n首先，MakeInit() 负责初始化函数的创建并插入 typecheck.Target.Inits，pkginit.Task() 得到了简化，毕竟这个初始化函数和其他用户定义的 init 实际上没有本质区别。\n其次，敏锐的同学可能发现了，类型检查的过程中，已经进行了一次 initOrder()，但只检查了循环依赖的问题；这次又 initOrder() 显得有些冗余。因此将这一部分从 Task() 中拆分出来，希望以后能够放到类型检查的过程中，避免重复的排序操作。当前抽离成了单独的函数但是还未并入类型检查，处于中间状态，可见不久后将会并入类型检查，注释中的 TODO 就是在说这个问题。\n最后，初始化函数如果在 Task() 中创建，则无法参与到类型检查结束到 Task() 开始这之间的优化过程，主要包括无效代码清理和内联优化。因此将其提前到类型检查结束后创建，这样就可以参与到优化过程中了。\npkginit.Task() 最后，终于来到了 init 处理的终点， pkginit.Task()。\n1// https://github.com/golang/go/blob/d8117459c513e048eb72f11988d5416110dff359/src/cmd/compile/internal/pkginit/init.go#L93 2// Task makes and returns an initialization record for the package. 3// See runtime/proc.go:initTask for its layout. 4// The 3 tasks for initialization are: 5// 1. Initialize all of the packages the current package depends on. 6// 2. Initialize all the variables that have initializers. 7// 3. Run any init functions. 8func Task() *ir.Name { 9 ... 10 // Find imported packages with init tasks. 11 // 这里可以看出 Package 最终的初始化任务被合并在了 .inittask 这个结构体中，因此对于引用的包才能这样进行查找，此处还检查了 .inittask 结构体是否合法。最终加入到 deps 数组。 12 for _, pkg := range typecheck.Target.Imports { 13 n := typecheck.Resolve(ir.NewIdent(base.Pos, pkg.Lookup(\u0026#34;.inittask\u0026#34;))) 14 if n.Op() == ir.ONONAME { 15 continue 16 } 17 if n.Op() != ir.ONAME || n.(*ir.Name).Class != ir.PEXTERN { 18 base.Fatalf(\u0026#34;bad inittask: %v\u0026#34;, n) 19 } 20 deps = append(deps, n.(*ir.Name).Linksym()) 21 } 22 ... 23 // 如果开启了 Address Sanitizer，那么需要在创建一个 init 函数加入 typecheck.Target.Inits，用于初始化 ASan 相关的全局变量。 24 if base.Flag.ASan { 25 ... 26 // 可见这个 init 将会在最后执行 27 typecheck.Target.Inits = append(typecheck.Target.Inits, fnInit) 28 } 29 ... 30 // Record user init functions. 31 for _, fn := range typecheck.Target.Inits { 32 // 只有处理 Package 全局变量的才叫 init，其它的都被重命名为了 init.0、init.1 等。 33 if fn.Sym().Name == \u0026#34;init\u0026#34; { 34 // Synthetic init function for initialization of package-scope 35 // variables. We can use staticinit to optimize away static 36 // assignments. 37 s := staticinit.Schedule{ 38 Plans: make(map[ir.Node]*staticinit.Plan), 39 Temps: make(map[ir.Node]*ir.Name), 40 } 41 for _, n := range fn.Body { 42 s.StaticInit(n) 43 } 44 fn.Body = s.Out 45 ir.WithFunc(fn, func() { 46 typecheck.Stmts(fn.Body) 47 }) 48 49 if len(fn.Body) == 0 { 50 fn.Body = []ir.Node{ir.NewBlockStmt(src.NoXPos, nil)} 51 } 52 } 53 54 // Skip init functions with empty bodies. 55 if len(fn.Body) == 1 { 56 if stmt := fn.Body[0]; stmt.Op() == ir.OBLOCK \u0026amp;\u0026amp; len(stmt.(*ir.BlockStmt).List) == 0 { 57 continue 58 } 59 } 60 fns = append(fns, fn.Nname.Linksym()) 61 } 最终 fns 数组保存了所有 init 函数；deps 数组保存了所有依赖的包的 .inittask 结构体。接下来合并构建自己 Package 的 .inittask。\n1// Make an .inittask structure. 2sym := typecheck.Lookup(\u0026#34;.inittask\u0026#34;) 3task := typecheck.NewName(sym) 4// 显然这个 .inittask 不是 uint8 类型的，只是为了占位，因此这里设置了一个 fake type。 5task.SetType(types.Types[types.TUINT8]) // fake type 6task.Class = ir.PEXTERN 7sym.Def = task 8lsym := task.Linksym() 9ot := 0 10// lsym.P = [0] 11ot = objw.Uintptr(lsym, ot, 0) // state: not initialized yet 12// lsym.P = [0, len(deps)] 13ot = objw.Uintptr(lsym, ot, uint64(len(deps))) 14// lsym.P = [0, len(deps), len(fns)] 15ot = objw.Uintptr(lsym, ot, uint64(len(fns))) 16// lsym.R = [newR(d)...] 17for _, d := range deps { 18 ot = objw.SymPtr(lsym, ot, d, 0) 19} 20// lsym.R = [newR(d)..., newR(f)...] 21for _, f := range fns { 22 ot = objw.SymPtr(lsym, ot, f, 0) 23} 24// An initTask has pointers, but none into the Go heap. 25// It\u0026#39;s not quite read only, the state field must be modifiable. 26// 此处说明这个 .inittask 符号是全局的，决定了最后在 object 文件中的位置区域。 27objw.Global(lsym, int32(ot), obj.NOPTR) 28return task 在最后将其设置为导出的（Export），因为其符号名并非大写字母开头，但是要被其他包使用：\n1// Build init task, if needed. 2if initTask := pkginit.Task(); initTask != nil { 3 typecheck.Export(initTask) 4} 至此，Package 单元对于 init 的处理就结束了，最后 Package 被编译为带有 .inittask 表的 object 文件，这个表中包含了所有的 init.x 函数和依赖的包的 .inittask 结构体指针，要注意这里只知道符号之间的关系，其他包里 init 函数的具体实现是不知道的，需要在链接阶段处理。\n链接时，在拥有了所有的 .inittask 包含的具体函数相关信息后，链接器会将其按照依赖关系进行排序，生成一个具体的 mainInittasks 列表供 runtime 使用。此处不再展开这一部分，有兴趣的同学可以自行阅读链接器 inittask 部分的源码：src/cmd/link/internal/ld/inittask.go，最终 SymbolName 为 go:main.inittasks。\n最终链接生成可执行文件时，inittasks 的地址会给到 src/runtime/proc.go 的 runtime_inittasks 数组变量，然后在runtime.main 函数中被使用：\n1// https://github.com/golang/go/blob/d8117459c513e048eb72f11988d5416110dff359/src/runtime/proc.go#L144 2func main() { 3 ... 4 doInit(runtime_inittasks) // Must be before defer. 5 ... 6} 最后回看一开始发现的现象：\n[现象]：a/b.go 和 a/c.go 的 init() 函数的执行顺序是按照文件名的字母顺序来的，将 a/b.go 改名后，其文件名顺序排在了 a/c.go 之后，最终 init() 执行也排在了之后。\n根源在于编译器在读取源文件时是按照文件系统文件名顺序读入，在处理时也是依文件次序处理的，也就是编译器遇到 init 和 import 的顺序都是由文件名顺序决定的。\n虽然有 initOrder() 的存在，但是它不会影响用户定义的 init() 的顺序\ninitOrder() 会处理 import 的依赖关系，因此最终各个 Package 的 init 顺序时根据依赖关系决定的。\n例如：\n1// a1.go 2import \u0026#34;b\u0026#34; 1// a2.go 2import \u0026#34;c\u0026#34; 1// b.go 2import \u0026#34;c\u0026#34; 那么不管 a1.go 和 a2.go 的文件名顺序如何，package c 都会先于 package b 初始化，因为 b 依赖 c。\n","permalink":"https://xinrea.cn/posts/go-init/","summary":"\u003cblockquote\u003e\n\u003cp\u003e本文中引用的源码均标注了 Golang 源码仓库链接，branch 为 \u003ccode\u003erelease-branch.go1.21\u003c/code\u003e（本文在编写时 Go 1.21 还未正式发布，正式版可能会有少量变化）。\u003c/p\u003e","title":"从 init 函数的顺序问题到 Go 编译器"},{"content":"在学习熟悉 CubismSDK 的时候，曾给轴伊Joi制作过一个简单的 Live2D 桌面宠物；由于是在官方样例的基础上进行的修改，因此程序主题通过 glew + glfw 来进行实现。由于桌面宠物的特殊性（需要尽可能减少对桌面操作的影响），可以说是必须实现异形窗口。这个异形窗口与一般的需求还不太一样：通常异形窗口是静态的，仅以一张图片作为底图，有很多种方法可以实现，其中一种便是用蒙版（Mask）来实现，但这种方式在桌面宠物这种场景下显得有点尴尬。\n对于用 OpenGL 动态渲染的桌面宠物来说，读取当前 Buffer 生成 Mask 是效率极低的。好在 Windows 下提供了 SetLayeredWindowAttributes(hwnd,RGB(0, 0, 0), 255, LWA_COLORKEY) 这一方法，直接进行键值抠图即可。但问题是其精度极低，渲染出的模型边缘会出现很明显的底色锯齿边缘。\n在 glfw 中，通过 glfwWindowHint(GLFW_TRANSPARENT_FRAMEBUFFER, GLFW_TRUE) 可实现窗口 Buffer 新增 Alpha 通道；配合 LWA_COLORKEY 即可实现无锯齿边缘的 OpenGL 渲染的即时异形窗口。\n然而最近在使用 Qt 对其重构的过程中，又遇到了异形窗口的这一问题。Qt 提供了两种使用 OpenGL 的方式，QOpenGLWindow 与 QOpenGLWidget；这两种方式在使用上几乎没有差异，可以很方便的互相转换。但在实现异形窗口的过程中遇到了问题。\n通过实践发现，QOpenGLWidget 通过设置 Qt::WA_TranslucentBackground 可以很简单的实现透明背景；但 LWA_COLORKEY 只对 WS_EX_LAYERED 样式的窗口生效；经过测试，QOpenGLWidget 单独作为窗口时，无法设置 WS_EX_LAYERED 样式，因此无法实现异形窗口。\nQOpenGLWindow 可以直接设置 WS_EX_LAYERED 样式，但无法设置透明背景（可使用 setFormat 添加 alpha 通道属性，但配合 LWA_COLORKEY 会显示异常）；因此需要自行实现 GLFW_TRANSPARENT_FRAMEBUFFER 的功能。\n通过阅读 glfw 源码，该设置相关的代码如下所示：\n1#define DWM_BB_ENABLE 0x00000001 2#define DWM_BB_BLURREGION 0x00000002 3typedef struct 4{ 5 DWORD dwFlags; 6 BOOL fEnable; 7 HRGN hRgnBlur; 8 BOOL fTransitionOnMaximized; 9} DWM_BLURBEHIND; 10 11typedef HRESULT(WINAPI * PFN_DwmEnableBlurBehindWindow)(HWND,const DWM_BLURBEHIND*); 12 13void setTransparentBuffer(HWND hwnd) 14{ 15 auto dll = LoadLibraryA(\u0026#34;dwmapi.dll\u0026#34;); 16 auto DwmEnableBlurBehindWindow = (PFN_DwmEnableBlurBehindWindow)GetProcAddress((HMODULE) dll, \u0026#34;DwmEnableBlurBehindWindow\u0026#34;); 17 HRGN region = CreateRectRgn(0, 0, -1, -1); 18 DWM_BLURBEHIND bb = {0}; 19 bb.dwFlags = DWM_BB_ENABLE | DWM_BB_BLURREGION; 20 bb.hRgnBlur = region; 21 bb.fEnable = TRUE; 22 23 DwmEnableBlurBehindWindow(hwnd, \u0026amp;bb); 24 DeleteObject(region); 25} 最终使用 QOpenGLWindow，手动设置透明 FrameBuffer，然后设置 LWA_COLORKEY，实现了完美的 OpenGL 内容的异形窗口。\n","permalink":"https://xinrea.cn/posts/qt-opengl-semi-window/","summary":"\u003cp\u003e在学习熟悉 CubismSDK 的时候，曾给轴伊Joi制作过一个简单的 Live2D 桌面宠物；由于是在官方样例的基础上进行的修改，因此程序主题通过 glew + glfw 来进行实现。由于桌面宠物的特殊性（需要尽可能减少对桌面操作的影响），可以说是必须实现异形窗口。这个异形窗口与一般的需求还不太一样：通常异形窗口是静态的，仅以一张图片作为底图，有很多种方法可以实现，其中一种便是用蒙版（Mask）来实现，但这种方式在桌面宠物这种场景下显得有点尴尬。\u003c/p\u003e","title":"Qt OpenglWindow 异形窗口的实现"},{"content":"最近在开发的项目用到了 gRPC，并且要求使用证书进行双向认证。于是便生成了一个 CA 证书，并以此签名生成了服务端和客户端所需的各种证书，且在周五进行客户端服务端连接测试时一切正常。周末两天过去了，周一再进行测试时，连接出现了如下错误:\ntransport: authentication handshake failed: EOF\n一开始还以为是证书验证出了问题，对相关证书以及涉及到的代码进行了回退，然后进行测试，仍然出现该问题。后续经过排查，排除了证书出错的这一原因，最后莫名在重启电脑后恢复了正常。后来发现是电脑上安装的梯子导致了这一问题。该问题的产生涉及到的前置原因如下：\n由于办公网络环境较差，于是临时安装梯子，搭配手机热点进行开发工作 由于需要使用证书双向认证，在证书 SANs 中添加了域名 *.sample.com 由于证书的使用需验证域名，因此本地测试时，修改 /etc/hosts 文件将相关域名解析到了本地地址 127.0.0.1 测试用的域名匹配了代理规则 由于使用域名测试连接，匹配了代理的规则，导致本地测试的连接无法建立，出现了上述的问题。由于客户端和服务端均在本地进行测试，因此一开始并未考虑连接问题，但最终发现问题是经梯子代理导致无法建立连接。\n","permalink":"https://xinrea.cn/posts/about-grpc-connection/","summary":"\u003cp\u003e最近在开发的项目用到了 gRPC，并且要求使用证书进行双向认证。于是便生成了一个 CA 证书，并以此签名生成了服务端和客户端所需的各种证书，且在周五进行客户端服务端连接测试时一切正常。周末两天过去了，周一再进行测试时，连接出现了如下错误:\u003c/p\u003e","title":"一个 gRPC 的连接问题"},{"content":" 哔哩哔哩舰长私信助手 BiliMessenger Vue Electron\n哔哩哔哩自动化私信工具，主要用于舰长礼物链接私信分发，提供了舰长列表获取的便捷途径。\n最开始是使用 C# 来开发的 UWP 应用，并在微软应用商店上架与更新，后来使用 Vue + Electron 重构。\n桌面宠物轴伊 JPet Live2D Cubism SDK Live2D C++ OpenGL\n虚拟主播轴伊的可互动多功能 Live2D 桌面宠物。\n想着熟悉一下 Live2D SDK 开发，于是在官方样例的基础上开发了这个桌面宠物。最开始使用自己绘画\u0026amp;建模的一个轴伊Joi_Channel的团子形象 Live2D 模型，后来 轴伊Joi_Channel 提供了自身 Q 版差分立绘，逐渐改进完成了桌面宠物。\n简易提问箱 JAsk Svelte Golang\n简单的提问箱，支持话题、图片投稿等小功能，同时拥有一个简单的后台以管理投稿。\nBilibili 弹幕机 JLiverTool VanillaJS HTML Electron\nBiliBili 弹幕机，除了常见的弹幕显示功能外，还支持礼物、醒目留言单独窗口显示，同时支持礼物记录本地保存管理。\nBiliShadowPlay Tauri Svelte Rust\nBiliBili 直播缓存及实时保存工具，功能类似 Nvidia ShadowPlay。\n","permalink":"https://xinrea.cn/projects/","summary":"projects","title":"Projects"}]